---
title: "Untitled"
author: "Janis Corona"
date: "4/2/2020"
output: html_document
---

Now that we have our table with the added ratios we can now look at building different data sets with feature selection to predict any of our other targets, but most likely the rating by review. But also we can now use our best prediction model of the mean votes algorithm and other big name machine learning algorithms I have used many times. We will do that next. [Caret Cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/caret.pdf) is available from the available cheatsheets in the toolbar in Rstudio in the Help menu under Cheatsheets, scroll to the 'Contributed Cheatsheets' at the bottom and select the caret cheatsheet if you would like to refresh or learn about the algorithms caret has to work with in R for machine learning on the above data set. 

Before moving on to the machine learning algorithms in R, specifically the caret package. Lets make a test and train set out of this data frame, and test the features we individually select on the target variable of the rating using our best model, the ceiling of the mean of the dot product of the votes and ratings or the top votes.We will use our database Reviews15.
If you don't have it in your environment go ahead and read it in from the ReviewsCleanedWithKeywordsAndRatios.csv file.
```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(visNetwork)
library(igraph)
library(tm)
library(e1071)
library(caret)
library(randomForest)
library(MASS)
library(gbm)
#install.packages('RANN')
library(RANN) #used in the tuning parameter of rf method of caret for 'oob' one out bag

```

```{r}

Reviews15 <- read.csv('ReviewsCleanedWithKeywordsAndRatios.csv',sep=',',
                      header=TRUE, na.strings=c('',' ','NA'))
wordToAllWords <- read.csv('wordToAllWords.csv', sep=',', header=TRUE, na.strings=c('',' ','NA'),
                           row.names=1)
```

Now lets see the table we will compare each review ratio of term to total terms to the term to total terms in all reviews by rating.
```{r}
head(wordToAllWords)
```

Now lets select our features we want to use the top model on. Since our model was based on the ratios, we will only be using the rating and those features, and from their it will create a voting system to grab the rating with the highest votes for the term to total terms per document. Make sure to read in all the packages this Rmarkdown file uses. The following will use stringr, dplyr, and tidyr or tidyverse for sure. Later we will use the caret package for Machine learning models to compare against our model.
```{r}
colnames(Reviews15)
```

```{r}
#ML_rating_data <- Reviews15[,c(5,33:43)]
ML_rating_data <- Reviews15 %>% select(userRatingValue,
                                       the_ratios:with_ratios)
head(ML_rating_data)
```


If, or when, we use the caret algorithms to make predictions with many of the available models in caret, the target variable would be the userRatingValue, and the keyword ratios (12) would be the predictors. But we aren't right at this moment and so we don't have to separate the target from the predictors just yet. We can keep it attached to compare to our model we build on the ceiling of the mean if a tie in votes for the minimum values of the review ratio to the five rating rations.

Keep the rating column as an integer instead of a factor, because we need it as an integer to use the dot product on those reviews that there is a tie against the votes (the minimum value of the difference between the term to total terms in the document to the ratios of the same for all the documents in each rating). We saw earlier that there are some instances where there is a tie, but that the ceiling of the mean or the highest rating will beat out the ceiling of the median or the shortest distance (absolute value) between the reveiw ratio and rating ratios. We need to also get our data frame on ratios of total of each of these 12 words or terms to the total of all words in each corpus or file of reviews by rating in the wordToAllWords table. 
```{r}
colnames(wordToAllWords)[3] <- 'for_' #this is special keyword in R
w2w <- wordToAllWords #shorten name
MLr <- ML_rating_data #shorten the name

MLr$R1_and <- rep(w2w[1,1],length(MLr$userRatingValue))
MLr$R2_and <- rep(w2w[2,1],length(MLr$userRatingValue))
MLr$R3_and <- rep(w2w[3,1],length(MLr$userRatingValue))
MLr$R4_and <- rep(w2w[4,1],length(MLr$userRatingValue))
MLr$R5_and <- rep(w2w[5,1],length(MLr$userRatingValue))

MLr$R1_but <- rep(w2w[1,2],length(MLr$userRatingValue))
MLr$R2_but <- rep(w2w[2,2],length(MLr$userRatingValue))
MLr$R3_but <- rep(w2w[3,2],length(MLr$userRatingValue))
MLr$R4_but <- rep(w2w[4,2],length(MLr$userRatingValue))
MLr$R5_but <- rep(w2w[5,2],length(MLr$userRatingValue))

MLr$R1_for <- rep(w2w[1,3],length(MLr$userRatingValue))
MLr$R2_for <- rep(w2w[2,3],length(MLr$userRatingValue))
MLr$R3_for <- rep(w2w[3,3],length(MLr$userRatingValue))
MLr$R4_for <- rep(w2w[4,3],length(MLr$userRatingValue))
MLr$R5_for <- rep(w2w[5,3],length(MLr$userRatingValue))

MLr$R1_good <- rep(w2w[1,4],length(MLr$userRatingValue))
MLr$R2_good <- rep(w2w[2,4],length(MLr$userRatingValue))
MLr$R3_good <- rep(w2w[3,4],length(MLr$userRatingValue))
MLr$R4_good <- rep(w2w[4,4],length(MLr$userRatingValue))
MLr$R5_good <- rep(w2w[5,4],length(MLr$userRatingValue))

MLr$R1_have <- rep(w2w[1,5],length(MLr$userRatingValue))
MLr$R2_have <- rep(w2w[2,5],length(MLr$userRatingValue))
MLr$R3_have <- rep(w2w[3,5],length(MLr$userRatingValue))
MLr$R4_have <- rep(w2w[4,5],length(MLr$userRatingValue))
MLr$R5_have <- rep(w2w[5,5],length(MLr$userRatingValue))

MLr$R1_not <- rep(w2w[1,6],length(MLr$userRatingValue))
MLr$R2_not <- rep(w2w[2,6],length(MLr$userRatingValue))
MLr$R3_not <- rep(w2w[3,6],length(MLr$userRatingValue))
MLr$R4_not <- rep(w2w[4,6],length(MLr$userRatingValue))
MLr$R5_not <- rep(w2w[5,6],length(MLr$userRatingValue))

MLr$R1_that <- rep(w2w[1,7],length(MLr$userRatingValue))
MLr$R2_that <- rep(w2w[2,7],length(MLr$userRatingValue))
MLr$R3_that <- rep(w2w[3,7],length(MLr$userRatingValue))
MLr$R4_that <- rep(w2w[4,7],length(MLr$userRatingValue))
MLr$R5_that <- rep(w2w[5,7],length(MLr$userRatingValue))

MLr$R1_the <- rep(w2w[1,8],length(MLr$userRatingValue))
MLr$R2_the <- rep(w2w[2,8],length(MLr$userRatingValue))
MLr$R3_the <- rep(w2w[3,8],length(MLr$userRatingValue))
MLr$R4_the <- rep(w2w[4,8],length(MLr$userRatingValue))
MLr$R5_the <- rep(w2w[5,8],length(MLr$userRatingValue))

MLr$R1_they <- rep(w2w[1,9],length(MLr$userRatingValue))
MLr$R2_they <- rep(w2w[2,9],length(MLr$userRatingValue))
MLr$R3_they <- rep(w2w[3,9],length(MLr$userRatingValue))
MLr$R4_they <- rep(w2w[4,9],length(MLr$userRatingValue))
MLr$R5_they <- rep(w2w[5,9],length(MLr$userRatingValue))

MLr$R1_this <- rep(w2w[1,10],length(MLr$userRatingValue))
MLr$R2_this <- rep(w2w[2,10],length(MLr$userRatingValue))
MLr$R3_this <- rep(w2w[3,10],length(MLr$userRatingValue))
MLr$R4_this <- rep(w2w[4,10],length(MLr$userRatingValue))
MLr$R5_this <- rep(w2w[5,10],length(MLr$userRatingValue))

MLr$R1_with <- rep(w2w[1,11],length(MLr$userRatingValue))
MLr$R2_with <- rep(w2w[2,11],length(MLr$userRatingValue))
MLr$R3_with <- rep(w2w[3,11],length(MLr$userRatingValue))
MLr$R4_with <- rep(w2w[4,11],length(MLr$userRatingValue))
MLr$R5_with <- rep(w2w[5,11],length(MLr$userRatingValue))

MLr$R1_you <- rep(w2w[1,12],length(MLr$userRatingValue))
MLr$R2_you <- rep(w2w[2,12],length(MLr$userRatingValue))
MLr$R3_you <- rep(w2w[3,12],length(MLr$userRatingValue))
MLr$R4_you <- rep(w2w[4,12],length(MLr$userRatingValue))
MLr$R5_you <- rep(w2w[5,12],length(MLr$userRatingValue))

MLr$and_diff1 <- MLr$R1_and-MLr$and_ratios
MLr$and_diff2 <- MLr$R2_and-MLr$and_ratios
MLr$and_diff3 <- MLr$R3_and-MLr$and_ratios
MLr$and_diff4 <- MLr$R4_and-MLr$and_ratios
MLr$and_diff5 <- MLr$R5_and-MLr$and_ratios

MLr$but_diff1 <- MLr$R1_but-MLr$but_ratios
MLr$but_diff2 <- MLr$R2_but-MLr$but_ratios
MLr$but_diff3 <- MLr$R3_but-MLr$but_ratios
MLr$but_diff4 <- MLr$R4_but-MLr$but_ratios
MLr$but_diff5 <- MLr$R5_but-MLr$but_ratios

MLr$for_diff1 <- MLr$R1_for-MLr$for_ratios 
MLr$for_diff2 <- MLr$R2_for-MLr$for_ratios 
MLr$for_diff3 <- MLr$R3_for-MLr$for_ratios 
MLr$for_diff4 <- MLr$R4_for-MLr$for_ratios 
MLr$for_diff5 <- MLr$R5_for-MLr$for_ratios 

MLr$good_diff1 <- MLr$R1_good-MLr$good_ratios
MLr$good_diff2 <- MLr$R2_good-MLr$good_ratios
MLr$good_diff3 <- MLr$R3_good-MLr$good_ratios
MLr$good_diff4 <- MLr$R4_good-MLr$good_ratios
MLr$good_diff5 <- MLr$R5_good-MLr$good_ratios

MLr$have_diff1 <- MLr$R1_have-MLr$have_ratios
MLr$have_diff2 <- MLr$R2_have-MLr$have_ratios
MLr$have_diff3 <- MLr$R3_have-MLr$have_ratios
MLr$have_diff4 <- MLr$R4_have-MLr$have_ratios
MLr$have_diff5 <- MLr$R5_have-MLr$have_ratios

MLr$not_diff1 <- MLr$R1_not-MLr$not_ratios
MLr$not_diff2 <- MLr$R2_not-MLr$not_ratios
MLr$not_diff3 <- MLr$R3_not-MLr$not_ratios
MLr$not_diff4 <- MLr$R4_not-MLr$not_ratios
MLr$not_diff5 <- MLr$R5_not-MLr$not_ratios

MLr$that_diff1 <- MLr$R1_that-MLr$that_ratios
MLr$that_diff2 <- MLr$R2_that-MLr$that_ratios
MLr$that_diff3 <- MLr$R3_that-MLr$that_ratios
MLr$that_diff4 <- MLr$R4_that-MLr$that_ratios
MLr$that_diff5 <- MLr$R5_that-MLr$that_ratios

MLr$the_diff1 <- MLr$R1_the-MLr$the_ratios
MLr$the_diff2 <- MLr$R2_the-MLr$the_ratios
MLr$the_diff3 <- MLr$R3_the-MLr$the_ratios
MLr$the_diff4 <- MLr$R4_the-MLr$the_ratios
MLr$the_diff5 <- MLr$R5_the-MLr$the_ratios

MLr$they_diff1 <- MLr$R1_they-MLr$they_ratios
MLr$they_diff2 <- MLr$R2_they-MLr$they_ratios
MLr$they_diff3 <- MLr$R3_they-MLr$they_ratios
MLr$they_diff4 <- MLr$R4_they-MLr$they_ratios
MLr$they_diff5 <- MLr$R5_they-MLr$they_ratios

MLr$this_diff1 <- MLr$R1_this-MLr$this_ratios
MLr$this_diff2 <- MLr$R2_this-MLr$this_ratios
MLr$this_diff3 <- MLr$R3_this-MLr$this_ratios
MLr$this_diff4 <- MLr$R4_this-MLr$this_ratios
MLr$this_diff5 <- MLr$R5_this-MLr$this_ratios

MLr$with_diff1 <- MLr$R1_with-MLr$with_ratios
MLr$with_diff2 <- MLr$R2_with-MLr$with_ratios
MLr$with_diff3 <- MLr$R3_with-MLr$with_ratios
MLr$with_diff4 <- MLr$R4_with-MLr$with_ratios
MLr$with_diff5 <- MLr$R5_with-MLr$with_ratios

MLr$you_diff1 <- MLr$R1_you-MLr$you_ratios
MLr$you_diff2 <- MLr$R2_you-MLr$you_ratios
MLr$you_diff3 <- MLr$R3_you-MLr$you_ratios
MLr$you_diff4 <- MLr$R4_you-MLr$you_ratios
MLr$you_diff5 <- MLr$R5_you-MLr$you_ratios



MLr$andMin <- apply(MLr[74:78],1, min)
MLr$andvote <- ifelse(MLr$R1_and==MLr$andMin,
                    1, 
                    ifelse(MLr$R2_and==MLr$andMin,
                           2,
                           ifelse(MLr$R3_and==MLr$andMin,
                                  3,
                                  ifelse(MLr$R4_and==MLr$andMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$butMin <- apply(MLr[79:83],1, min)
MLr$butvote <- ifelse(MLr$R1_but==MLr$butMin,
                    1, 
                    ifelse(MLr$R2_but==MLr$butMin,
                           2,
                           ifelse(MLr$R3_but==MLr$butMin,
                                  3,
                                  ifelse(MLr$R4_but==MLr$butMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$forMin <- apply(MLr[84:88],1, min)
MLr$forvote <- ifelse(MLr$R1_for==MLr$forMin,
                    1, 
                    ifelse(MLr$R2_for==MLr$forMin,
                           2,
                           ifelse(MLr$R3_for==MLr$forMin,
                                  3,
                                  ifelse(MLr$R4_for==MLr$forMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$goodMin <- apply(MLr[89:93],1, min)
MLr$goodvote <- ifelse(MLr$R1_good==MLr$goodMin,
                    1, 
                    ifelse(MLr$R2_good==MLr$goodMin,
                           2,
                           ifelse(MLr$R3_good==MLr$goodMin,
                                  3,
                                  ifelse(MLr$R4_good==MLr$goodMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$haveMin <- apply(MLr[94:98],1, min)
MLr$havevote <- ifelse(MLr$R1_have==MLr$haveMin,
                    1, 
                    ifelse(MLr$R2_have==MLr$haveMin,
                           2,
                           ifelse(MLr$R3_have==MLr$haveMin,
                                  3,
                                  ifelse(MLr$R4_have==MLr$haveMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$notMin <- apply(MLr[99:103],1, min)
MLr$notvote <- ifelse(MLr$R1_not==MLr$notMin,
                    1, 
                    ifelse(MLr$R2_not==MLr$notMin,
                           2,
                           ifelse(MLr$R3_not==MLr$notMin,
                                  3,
                                  ifelse(MLr$R4_not==MLr$notMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$thatMin <- apply(MLr[104:108],1, min)
MLr$thatvote <- ifelse(MLr$R1_that==MLr$thatMin,
                    1, 
                    ifelse(MLr$R2_that==MLr$thatMin,
                           2,
                           ifelse(MLr$R3_that==MLr$thatMin,
                                  3,
                                  ifelse(MLr$R4_that==MLr$thatMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$theMin <- apply(MLr[109:113],1, min)
MLr$thevote <- ifelse(MLr$R1_the==MLr$theMin,
                    1, 
                    ifelse(MLr$R2_the==MLr$theMin,
                           2,
                           ifelse(MLr$R3_the==MLr$theMin,
                                  3,
                                  ifelse(MLr$R4_the==MLr$theMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$theyMin <- apply(MLr[114:118],1, min)
MLr$theyvote <- ifelse(MLr$R1_they==MLr$theyMin,
                    1, 
                    ifelse(MLr$R2_they==MLr$theyMin,
                           2,
                           ifelse(MLr$R3_they==MLr$theyMin,
                                  3,
                                  ifelse(MLr$R4_they==MLr$theyMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$thisMin <- apply(MLr[119:123],1, min)
MLr$thisvote <- ifelse(MLr$R1_this==MLr$thisMin,
                    1, 
                    ifelse(MLr$R2_this==MLr$thisMin,
                           2,
                           ifelse(MLr$R3_this==MLr$thisMin,
                                  3,
                                  ifelse(MLr$R4_this==MLr$thisMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$withMin <- apply(MLr[124:128],1, min)
MLr$withvote <- ifelse(MLr$R1_with==MLr$withMin,
                    1, 
                    ifelse(MLr$R2_with==MLr$withMin,
                           2,
                           ifelse(MLr$R3_with==MLr$withMin,
                                  3,
                                  ifelse(MLr$R4_with==MLr$withMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$youMin <- apply(MLr[129:133],1, min)
MLr$youvote <- ifelse(MLr$R1_you==MLr$youMin,
                    1, 
                    ifelse(MLr$R2_you==MLr$youMin,
                           2,
                           ifelse(MLr$R3_you==MLr$youMin,
                                  3,
                                  ifelse(MLr$R4_you==MLr$youMin,
                                         4,
                                         5)
                                  )
                           )
                    )


```

```{r}
bestVote <- MLr %>% select(andvote, 
      butvote, forvote, 
      goodvote, havevote,
      notvote,  thatvote,  thevote,theyvote,
      thisvote,withvote,
      youvote)
      
bestVote$andvote <- as.factor(paste(bestVote$andvote)) 
bestVote$butvote <- as.factor(paste(bestVote$butvote)) 
bestVote$forvote <- as.factor(paste(bestVote$forvote)) 
bestVote$goodvote <- as.factor(paste(bestVote$goodvote)) 
bestVote$havevote <- as.factor(paste(bestVote$havevote))
bestVote$notvote <- as.factor(paste(bestVote$notvote))  
bestVote$thatvote <- as.factor(paste(bestVote$thatvote))                                  
bestVote$thevote <- as.factor(paste(bestVote$thevote))
bestVote$theyvote <- as.factor(paste(bestVote$theyvote))
bestVote$thisvote <- as.factor(paste(bestVote$thisvote))
bestVote$withvote <- as.factor(paste(bestVote$withvote))
bestVote$youvote <- as.factor(paste(bestVote$youvote))

bestVote$counts1 <- 0
bestVote$counts2 <- 0
bestVote$counts3 <- 0
bestVote$counts4 <- 0
bestVote$counts5 <- 0

a5 <- grep('5',bestVote$andvote)
a4 <- grep('4', bestVote$andvote)
a3 <- grep('3',bestVote$andvote)
a2 <- grep('2',bestVote$andvote)
a1 <- grep('1',bestVote$andvote)

b5 <- grep('5',bestVote$butvote)
b4 <- grep('4', bestVote$butvote)
b3 <- grep('3',bestVote$butvote)
b2 <- grep('2',bestVote$butvote)
b1 <- grep('1',bestVote$butvote)

c5 <- grep('5',bestVote$forvote)
c4 <- grep('4', bestVote$forvote)
c3 <- grep('3',bestVote$forvote)
c2 <- grep('2',bestVote$forvote)
c1 <- grep('1',bestVote$forvote)

d5 <- grep('5',bestVote$goodvote)
d4 <- grep('4', bestVote$goodvote)
d3 <- grep('3',bestVote$goodvote)
d2 <- grep('2',bestVote$goodvote)
d1 <- grep('1',bestVote$goodvote)

e5 <- grep('5',bestVote$havevote)
e4 <- grep('4', bestVote$havevote)
e3 <- grep('3',bestVote$havevote)
e2 <- grep('2',bestVote$havevote)
e1 <- grep('1',bestVote$havevote)

f5 <- grep('5',bestVote$notvote)
f4 <- grep('4', bestVote$notvote)
f3 <- grep('3',bestVote$notvote)
f2 <- grep('2',bestVote$notvote)
f1 <- grep('1',bestVote$notvote)

g5 <- grep('5',bestVote$thatvote)
g4 <- grep('4', bestVote$thatvote)
g3 <- grep('3',bestVote$thatvote)
g2 <- grep('2',bestVote$thatvote)
g1 <- grep('1',bestVote$thatvote)

h5 <- grep('5',bestVote$thevote)
h4 <- grep('4', bestVote$thevote)
h3 <- grep('3',bestVote$thevote)
h2 <- grep('2',bestVote$thevote)
h1 <- grep('1',bestVote$thevote)

i5 <- grep('5',bestVote$theyvote)
i4 <- grep('4', bestVote$theyvote)
i3 <- grep('3',bestVote$theyvote)
i2 <- grep('2',bestVote$theyvote)
i1 <- grep('1',bestVote$theyvote)

j5 <- grep('5',bestVote$thisvote)
j4 <- grep('4', bestVote$thisvote)
j3 <- grep('3',bestVote$thisvote)
j2 <- grep('2',bestVote$thisvote)
j1 <- grep('1',bestVote$thisvote)

k5 <- grep('5',bestVote$withvote)
k4 <- grep('4', bestVote$withvote)
k3 <- grep('3',bestVote$withvote)
k2 <- grep('2',bestVote$withvote)
k1 <- grep('1',bestVote$withvote)

l5 <- grep('5',bestVote$youvote)
l4 <- grep('4', bestVote$youvote)
l3 <- grep('3',bestVote$youvote)
l2 <- grep('2',bestVote$youvote)
l1 <- grep('1',bestVote$youvote)

bestVote$counts1[l1] <- bestVote$counts1[l1]+ 1
bestVote$counts1[k1] <- bestVote$counts1[k1]+ 1
bestVote$counts1[j1] <- bestVote$counts1[j1]+ 1
bestVote$counts1[i1] <- bestVote$counts1[i1]+ 1
bestVote$counts1[h1] <- bestVote$counts1[h1]+ 1
bestVote$counts1[g1] <- bestVote$counts1[g1]+ 1
bestVote$counts1[f1] <- bestVote$counts1[f1]+ 1
bestVote$counts1[e1] <- bestVote$counts1[e1]+ 1
bestVote$counts1[d1] <- bestVote$counts1[d1]+ 1
bestVote$counts1[c1] <- bestVote$counts1[c1]+ 1
bestVote$counts1[b1] <- bestVote$counts1[b1]+ 1
bestVote$counts1[a1] <- bestVote$counts1[a1]+ 1

bestVote$counts2[l2]  <- bestVote$counts2[l2] + 1
bestVote$counts2[k2]  <- bestVote$counts2[k2] + 1
bestVote$counts2[j2]  <- bestVote$counts2[j2] + 1
bestVote$counts2[i2]  <- bestVote$counts2[i2] + 1
bestVote$counts2[h2]  <- bestVote$counts2[h2] + 1
bestVote$counts2[g2]  <- bestVote$counts2[g2] + 1
bestVote$counts2[f2]  <- bestVote$counts2[f2] + 1
bestVote$counts2[e2]  <- bestVote$counts2[e2] + 1
bestVote$counts2[d2]  <- bestVote$counts2[d2] + 1
bestVote$counts2[c2]  <- bestVote$counts2[c2] + 1
bestVote$counts2[b2]  <- bestVote$counts2[b2] + 1
bestVote$counts2[a2]  <- bestVote$counts2[a2] + 1

bestVote$counts3[l3]  <- bestVote$counts3[l3] + 1
bestVote$counts3[k3]  <- bestVote$counts3[k3] + 1
bestVote$counts3[j3]  <- bestVote$counts3[j3] + 1
bestVote$counts3[i3]  <- bestVote$counts3[i3] + 1
bestVote$counts3[h3]  <- bestVote$counts3[h3] + 1
bestVote$counts3[g3]  <- bestVote$counts3[g3] + 1
bestVote$counts3[f3]  <- bestVote$counts3[f3] + 1
bestVote$counts3[e3]  <- bestVote$counts3[e3] + 1
bestVote$counts3[d3]  <- bestVote$counts3[d3] + 1
bestVote$counts3[c3]  <- bestVote$counts3[c3] + 1
bestVote$counts3[b3]  <- bestVote$counts3[b3] + 1
bestVote$counts3[a3]  <- bestVote$counts3[a3] + 1

bestVote$counts4[l4]  <- bestVote$counts4[l4] + 1
bestVote$counts4[k4]  <- bestVote$counts4[k4] + 1
bestVote$counts4[j4]  <- bestVote$counts4[j4] + 1
bestVote$counts4[i4]  <- bestVote$counts4[i4] + 1
bestVote$counts4[h4]  <- bestVote$counts4[h4] + 1
bestVote$counts4[g4]  <- bestVote$counts4[g4] + 1
bestVote$counts4[f4]  <- bestVote$counts4[f4] + 1
bestVote$counts4[e4]  <- bestVote$counts4[e4] + 1
bestVote$counts4[d4]  <- bestVote$counts4[d4] + 1
bestVote$counts4[c4]  <- bestVote$counts4[c4] + 1
bestVote$counts4[b4]  <- bestVote$counts4[b4] + 1
bestVote$counts4[a4]  <- bestVote$counts4[a4] + 1

bestVote$counts5[l5]  <- bestVote$counts5[l5] + 1
bestVote$counts5[k5]  <- bestVote$counts5[k5] + 1
bestVote$counts5[j5]  <- bestVote$counts5[j5] + 1
bestVote$counts5[i5]  <- bestVote$counts5[i5] + 1
bestVote$counts5[h5]  <- bestVote$counts5[h5] + 1
bestVote$counts5[g5]  <- bestVote$counts5[g5] + 1
bestVote$counts5[f5]  <- bestVote$counts5[f5] + 1
bestVote$counts5[e5]  <- bestVote$counts5[e5] + 1
bestVote$counts5[d5]  <- bestVote$counts5[d5] + 1
bestVote$counts5[c5]  <- bestVote$counts5[c5] + 1
bestVote$counts5[b5]  <- bestVote$counts5[b5] + 1
bestVote$counts5[a5]  <- bestVote$counts5[a5] + 1


bestVote$maxVote <- apply(bestVote[13:17],1,max)

mv <- bestVote$maxVote
ct1 <- bestVote$counts1
ct2 <- bestVote$counts2
ct3 <- bestVote$counts3
ct4 <- bestVote$counts4
ct5 <- bestVote$counts5


bestVote$votedRating <- ifelse(mv==ct1, 1, 
                        ifelse(mv==ct2, 2,
                        ifelse(mv==ct3, 3,
                        ifelse(mv==ct4, 4, 5))))

bestVote$Rating <- ifelse(mv==ct1 & (mv==ct2|mv==ct3|mv==ct4|mv==ct5),'tie',
                     ifelse(mv==ct1,1, 
                           ifelse(mv==ct2 & (mv==ct3|mv==ct4|mv==ct5),'tie',
                     ifelse(mv==ct2, 2,
                           ifelse(mv==ct3 &(mv==ct4|mv==ct5),'tie', 
                           ifelse(mv==ct3, 3,
                           ifelse(mv==ct4 & mv==ct5, 'tie',
                           ifelse(mv==ct4, 4,5
                           )))))))
                          )
bestVote$finalPrediction <- ifelse(bestVote$Rating=='tie', 
                     ifelse(ceiling((c(ct1,ct2,ct3,ct4,ct5)*c(1,2,3,4,5))/5) > 5,
                     5, ceiling((c(ct1,ct2,ct3,ct4,ct5)*c(1,2,3,4,5))/5)), 
                     bestVote$votedRating )

```


Now that we have our final prediction with this algorithm. Lets attach these two tables together and rearrange the columns.
```{r}
MLr2 <- cbind(MLr, bestVote)
MLr3 <- MLr2[,c(2:178,1)]
MLr3$CorrectPrediction <- ifelse(MLr3$finalPrediction==MLr3$userRatingValue,
                                 1,0)
MLr3$finalPrediction <- as.factor(paste(MLr3$finalPrediction))
MLr3$userRatingValue <- paste('rating ', MLr3$userRatingValue,sep='')
Accuracy <- sum(MLr3$CorrectPrediction)/length(MLr3$CorrectPrediction)
Accuracy
```

The first bunch of reviews seem to be very accurate, but then more reviews get less accurate. 
```{r}
MLr3[1:20,175:179]
```

This was a good approach to building a model from the ground up and developing algorithms that seem like they could be good models to make predictions with. This was a straight algorithm model as in a function you put something in and get an answer out. The caret package has other more heavily used and industry related models tht are used frequently and can be tune. We will look at those later.

Lets write this table out to csv.
```{r}
write.csv(MLr3, 'ML_Reviews614_resultsTable.csv', row.names=FALSE)
```

Lets look at those values that the correct prediction was false.
```{r}
FalsePredictions <- subset(MLr3, MLr3$CorrectPrediction==0)
head(FalsePredictions[,175:179],30)
```
Many times the voted rating was not the rating even if it wasn't a tie. The 2-4 ratings were most of the incorrect predictions and the rating as being either a 5 or a 1 were more 50/50 probably because their ratios of term to total terms were very close. Some tuning that could be done would be to select different keywords, see if there is a difference between type of business or cost of service or goods. We left in these words of which most are included in stopwords like by,my,has,have,etc.that are personal pronouns or  of the text mining tm package. 
```{r}
#library(tm)
stop <- stopwords()
stop
```
The above are the stopwords that are eliminated within text cleaning and preprocessing before running or retrieving a document term matrix for an observation such as a review.
Now lets look at are keywords.
```{r}
keywordsUsed <- colnames(wordToAllWords)
keywordsUsed
```
The above shows our keywords, and note that for is for_ because it errors in R as it is a programming keyword so it was altered in the column name with an appended underscore character, but the search for it as a character or factor is 'for' not 'for_'. We actually see that all 12 of our stopwords are keywords. We briefly explained that this is because in grammar and composition to write a persuasive stories many connections have to be made with the use of pronouns and actions and states of being and descriptors. These were used as features by count to see if they added any value to predicting a review. We do see that these stop words are used a lot for extreme ends of the rating scale as a 1 for the lowest and 5 for the highest review rating.

Lets get a count of each rating by incorrect classification.
```{r}
userRatings <- FalsePredictions %>% group_by(userRatingValue) %>% count(finalPrediction)
userRatings
```
We can see from the userRatingValue and the final prediction, that there were 85 instances
where this model couldn't distinguish between a 1 or 5 rating based on the keywords (mostly stopwords). But also those ratings that were 4s were actually classified the most incorrectly as a 5 because some people don't readily give 5s as others. Also the actual 2s that were classified incorrectly were scaled up to 5s in the error, the same with the 3s. The gray areas were in user ratings of 2-4, where few were classified as a value in a 2-4 that was incorrect. If we instead said to classify any 2-4 as in the range of 2-4 without penalizing the exact value, then the prediction accuracy would be better. But people have their own reasons for giving 2-4s in ratings. Like they had better or there was an absolute worst experience the company still doesn't meet because he or she knows it could be worse and it could be better. Companies are supposed to use this to improve or adjust needs of consumers. But at the same time some users are just upset with the cost or time wasted or spent when other options they know of were or are better.

Lets look at those predictions with a tie.
```{r}
ties <- subset(MLr3, MLr3$Rating=='tie')
ties[,175:179]
```
There weren't a lot of ties on votes for the minimum difference of review to all reviews by ratios of term to total terms.
```{r}
tieGroups <- ties %>% group_by(userRatingValue) %>% count(finalPrediction)
tieGroups
```

Note that these tie ratings are both correct and incorrect. From the above grouped information of counts of user ratings by final predictions all the 5s were misclassified or predicted to be in the gray area of 2-4 and more of the 1s were classified correctly except for one review in the middle gray area. None of the 2s are in our list of ties but many of the gray area 2-4s were classified into the gray area of incorrect 2-4s such as the 4s and 3s.

We could use a link analysis to see how these results compared with the final predicted value and actual rating value as groups. With the nodes as the user rating, and edges as the final prediction. We should also add in one of either the business type or the cost.
First lets combine the Reviews15 table with the MLr3 table of only the results.
```{r}
MLr4 <- MLr3 %>% select(maxVote:CorrectPrediction)
MLr4$actualRatingValue <- MLr4$userRatingValue
MLr5 <- MLr4 %>% select(-userRatingValue)
Reviews15_results <- cbind(Reviews15, MLr5)
colnames(Reviews15_results)
```
Lets keep the ratios because this table minus results (and the review content columns) just added will be useful when running the caret algorithms. For now, lets write this out to csv to easily read in later instead of running whatever chunks preceded this table to get it.
```{r}
write.csv(Reviews15_results, 'Reviews15_results.csv', row.names=FALSE)
```

Now lets use visNetwork to group by CorrectPrediction and map the actualRatingValue to the predicted values as well as add a hovering feature as the title column in our nodes table of the businessType. We have to pick a width for the weighted arrows, or else there won't be any edges. Lets pick the not_ratios. 
```{r}
network <- Reviews15_results %>% select(businessType,
                                        finalPrediction:actualRatingValue, not,
                                        not_ratios)

network$finalPrediction <- paste('predicted',network$finalPrediction,sep=' ')
network$CorrectPrediction <- gsub(1,'TRUE', network$CorrectPrediction)
network$CorrectPrediction <- gsub(0,'FALSE', network$CorrectPrediction)
head(network,20)
```

The nodes table will include all of the above except the not_ratios.
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$title <- nodes$businessType
nodes$label <- nodes$actualRatingValue
nodes$group <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label, title,group,finalPrediction)
head(nodes1,10)
```

The edges will have the finalPrediction and actualRatingValue columns.
```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(finalPrediction,actualRatingValue,not,not_ratios)
edges$label <- edges$finalPrediction
edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$actualRatingValue,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$finalPrediction,
                                                   from = nodes$finalPrediction,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width,weight)
head(edges3,10)
```

Lets see the link analysis visualization.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions of True or False Ratings') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```

The above shows a visual network that looks mostly half and half by true versus false predicted rating values.None of the edges show except for that tiny cluster above, that shows the predicted ratings.


Lets group by business type now and map the actual to correctly predicted
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$group <- nodes$businessType
nodes$label <- nodes$actualRatingValue
nodes$title <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label, title,group,finalPrediction)
head(nodes1,10)
```

The edges will have the finalPrediction and actualRatingValue columns.
```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(finalPrediction,actualRatingValue,not,not_ratios)
edges$label <- edges$finalPrediction
edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$actualRatingValue,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$finalPrediction,
                                                   from = nodes$finalPrediction,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width, weight)
head(edges3,10)
```

Lets see the link analysis visualization.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions of Business Type Ratings with True or False and Actual Rating') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```


Lets group by business type now and map the business type to the prediction.
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$group <- nodes$businessType
nodes$label <- nodes$businessType
nodes$title <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label,group,finalPrediction, title)
head(nodes1,10)
```

```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(CorrectPrediction,actualRatingValue,not,businessType,
                            finalPrediction)
edges$label <- edges$finalPrediction
#edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$businessType,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$CorrectPrediction,
                                                   from = nodes$title,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width)
head(edges3,10)
```

Lets see the link analysis visualization with the star layout.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout.star') %>%
  visLegend
```
The above is a hula hoop because there are not any links for the most part, and only a handfule of the true or false predictions are mapping from business type to other business types by other business types predicted true or false. you can click on a node drag it off the disk and see no links, but click on the background to stop dragging. Then do the same with the center nodes that do have links to the disk and see the edges stay attached with the predicted value. Not many business types had the same predicted value as a true or false prediction of the same actual to predicted by business type.

Lets see this design in a grid layout. The sphere is the default and you can see it doesn't really describe much visually without grouping when there are no links between the nodes.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout_on_grid') %>%
  visLegend
```

The above is a grid layout that looks like tetris or pac man ping pong machine type layout with the groups.

Lets look out the layout.graphopt layout next.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout.graphopt') %>%
  visLegend
```

Looks a lot like the default layout for this data of features. This is the spherical layout that follows.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout_on_sphere') %>%
  visLegend
```

It looks similar to the hula hoop star layout for this data with minimal links between nodes.


***
***
***
 That was interesting to look at the different layouts in igraph with visNetwork, and to also see ways to improve our model design just as the businesses use reviews to improve or make changes as needed by analyzing the correctly and falsely predicted ratings. The next section will use machine learning algorithms in the 
 [caret package](https://github.com/rstudio/cheatsheets/raw/master/caret.pdf)to test out results of various models and defaults as well as test out the attributes within each model for tuning and validating for better generalization. 
 
Lets use the Reviews15_results table, you can read it in if you closed your RStudio session and emptied your environment. The file we saved it as is the ML_Reviews614_resultsTable.csv file.
```{r}
colnames(Reviews15_results)

```

The column features to start with in predicting ratings will be the userRatingValue, businessReplied as a yes or no, LowAvgHighCost, businessType, number of friends, number of reviews, number of photos, number of userBusinessPhotos, weekday, number of userCheckIns, if the user is an eliteStatus, and the stopword ratios.Make sure your libraries are loaded, we will use the caret package to run some analysis on this table of features.
```{r}
businessRatings <- Reviews15_results %>% select(userRatingValue,businessReplied,
                                                LowAvgHighCost,businessType,
                                                friends ,             
 reviews  ,            photos      ,          eliteStatus   ,       
   userBusinessPhotos   ,
 userCheckIns ,         weekday ,   the_ratios,           
 and_ratios   ,         for_ratios  ,          have_ratios  ,        
 that_ratios  ,         they_ratios   ,        this_ratios ,         
 you_ratios   ,         not_ratios    ,        but_ratios ,          
 good_ratios  ,         with_ratios                  )

businessRatings$userRatingValue <- as.factor(paste(businessRatings$userRatingValue))
head(businessRatings)
```

Lets use the numeric fields to predict the target of the ratings.
```{r}
numRegressions <- businessRatings %>% select(userRatingValue,
                     friends:photos, userBusinessPhotos,userCheckIns,
                     the_ratios:with_ratios)
numRegressions$userRatingValue <- as.numeric(paste(numRegressions$userRatingValue))
str(numRegressions)
```

Now lets select our training set and our testing set to build the caret models and test the models on. We will sample randomly with the sample function on our indices of the training set and use those indices not in the training set for our testing set.
```{r}
set.seed(56789)
train <- sample(floor(.7*length(numRegressions$userRatingValue)),replace=FALSE)

trainingSet <- numRegressions[train,]
testingSet <- numRegressions[-train,]

dim(trainingSet);dim(testingSet);dim(trainingSet)[1]+dim(testingSet)[1];dim(numRegressions)
```
Optionally you could use this method
```{r}
inTrain <- createDataPartition(y=numRegressions$userRatingValue, p=0.7, list=FALSE)

trainingSet2 <- numRegressions[inTrain,]
testingSet2 <- numRegressions[-inTrain,]


```


Our training set to build the model is 429 reviews, and our testing set is 185 reviews.
There are more 5s in the data overall. Lets look at those numbers.
```{r}
statsTrain <- trainingSet %>% group_by(userRatingValue) %>% count()
statsTrain$percent <- statsTrain$n/sum(statsTrain$n)
statsTrain
```

```{r}
statsTest <- testingSet %>% group_by(userRatingValue) %>% count()
statsTest$percent <- statsTest$n/sum(statsTest$n)
statsTest
```
There are more percent of 3s and 5s in the testing set and more 1s,2s, and 4s in the training set. but not by too much.

Lets see what the percent of sampling is with the createDataPartition function in the second sampled set.
```{r}
statsTrain2 <- trainingSet2 %>% group_by(userRatingValue) %>% count()
statsTrain2$percent <- statsTrain2$n/sum(statsTrain2$n)
statsTrain2
```

```{r}
statsTest2 <- testingSet2 %>% group_by(userRatingValue) %>% count()
statsTest2$percent <- statsTest2$n/sum(statsTest2$n)
statsTest2

```

The number of percents are better with the createDataPartitions function, so we will use that sampling set.

```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod0 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet2),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```

The following produces an error because of the imputing of missing data, when running the next line the predRF0 only has 15 rows, but the testingSet2 has 183, it only predicted by the features that were available.
```{r,eval=FALSE}
predRF0 <- predict(rfMod0, testingSet2)

predDF0 <- data.frame(predRF0, type=testingSet2$userRatingValue)
predDF0

sum <- sum(predRF0==testingSet2$userRatingValue) 
length <- length(testingSet2$userRatingValue)
accuracy_rfMod0 <- (sum/length) 
accuracy_rfMod0

```


Lets just use the data set without the meta data as many values are missing. We need a new data table. We will just remove the features we don't need from our testingSet2 and trainingSet2 tables. Some of the supplemental packages to caret when tuning the random forest trees interferes with tidyverse packages, so we'll use slicing.
```{r}
trainingSet3 <- trainingSet2[,c(1,7:18)]
trainingSet3$userRatingValue <- as.factor(paste(trainingSet3$userRatingValue))

# trainingSet3 <- trainingSet2 %>% select(-friends, -reviews, -photos,
#                                         -userBusinessPhotos,-userCheckIns)
colnames(trainingSet3)
```
```{r}
testingSet3 <- testingSet2[,c(1,7:18)]
testingSet3$userRatingValue <- as.factor(paste(testingSet3$userRatingValue))
# testingSet3 <- testingSet2 %>% select(-friends, -reviews, -photos,
#                                         -userBusinessPhotos,-userCheckIns)
colnames(testingSet3)
```

```{r}
dim(testingSet3);dim(trainingSet3)
```
Lets see if it works for the caret rf model this time.
```{r, error=FALSE, message=FALSE, warning=FALSE}
# requires the RANN package

rfMod1 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","knnImpute"),
               trControl=trainControl(method='oob'), number=5)
```

It does! 
```{r}
predRF1 <- predict(rfMod1, testingSet3)

predDF1 <- data.frame(predRF1, type=testingSet3$userRatingValue)
predDF1

sum1 <- sum(predRF1==testingSet3$userRatingValue) 
length1 <- length(testingSet3$userRatingValue)
accuracy_rfMod1 <- (sum1/length1) 
accuracy_rfMod1

```
We see that the above is regressing with the random forest, lets change the target to a factor.
```{r}
trainingSet3$userRatingValue <- as.factor(paste(trainingSet3$userRatingValue))
testingSet3$userRatingValue <- as.factor(paste(testingSet3$userRatingValue))
```

Lets re-run the above two chunks of the model and predictions to see the results.
```{r, error=FALSE, message=FALSE, warning=FALSE}
# requires the RANN package

rfMod1 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","knnImpute"),
               trControl=trainControl(method='oob'), number=5)
```

```{r}
predRF1 <- predict(rfMod1, testingSet3)

predDF1 <- data.frame(predRF1, type=testingSet3$userRatingValue)
predDF1

sum1 <- sum(predRF1==testingSet3$userRatingValue) 
length1 <- length(testingSet3$userRatingValue)
accuracy_rfMod1 <- (sum1/length1) 
head(accuracy_rfMod1,30)

```
The results are much better and actually the similar range of half and half as our manually built model by using just the stopwords that appeared the most to predict the rating. The above model also used the one out bagging method for trees in validating, and it preprocessed by centering and scaling the variables to normalize them as well as set to imputing NAs with knnImpute, even though there weren't any NAs in the ratios, just some zeros.


This next model uses the bagImpute for NAs and with the same number, 5, of iterations to one out bag validation.
```{r,message=FALSE, error=FALSE, warning=FALSE}
rfMod2 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","bagImpute"),
               trControl=trainControl(method='oob'), number=5)
```

```{r}
predRF2 <- predict(rfMod2, testingSet3)

predDF2 <- data.frame(predRF2, type=testingSet3$userRatingValue)
predDF2

sum2 <- sum(predRF2==testingSet3$userRatingValue) 
length2 <- length(testingSet3$userRatingValue)
accuracy_rfMod2 <- (sum2/length2) 
head(accuracy_rfMod2,30)

```

The results of the bagImpute random forest classification is two percent worse than the knnImpute with the same settings in accuracy.


This next random forest model uses the bootstrap method set to five iterations and medianImpute of NAs.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod3 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='boot'), number=5)
```

```{r}
predRF3 <- predict(rfMod3, testingSet3)

predDF3 <- data.frame(predRF3, type=testingSet3$userRatingValue)
predDF3

sum3 <- sum(predRF3==testingSet3$userRatingValue) 
length3 <- length(testingSet3$userRatingValue)
accuracy_rfMod3 <- (sum3/length3) 
head(accuracy_rfMod3,30)

```
The results with the bootstrap method and median imputing of NAs is almost as good as the knnImpute of NAs with the one out bag. Both at 56% but the knnImpute 6/10 percent better.

Lets use the bootstrap method with knnImpute on the next random forest model.
```{r,error=FALSE, message=FALSE,warning=FALSE}
rfMod4 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","knnImpute"),
               trControl=trainControl(method='boot'), number=5)
```

```{r}
predRF4 <- predict(rfMod4, testingSet3)

predDF4 <- data.frame(predRF4, type=testingSet3$userRatingValue)
predDF4

sum4 <- sum(predRF4==testingSet3$userRatingValue) 
length4 <- length(testingSet3$userRatingValue)
accuracy_rfMod4 <- (sum4/length4) 
head(accuracy_rfMod4,30)

```
The bootstrap with knnImpute above did exactly the same as the medianImpute bootstrap random forest as they both predicted 103/183 correct. The knnImpute with one out bag did the best so far with 104/183. The lowest scoring imputing was bagImpute in the one out bag. Lets see this NA imputation with the adaptive_cv method.

Lets use the 
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod5 <- train(userRatingValue~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","bagImpute"),
               trControl=trainControl(method='adaptive_cv'), number=5)
```

```{r}
predRF5 <- predict(rfMod5, testingSet3)

predDF5 <- data.frame(predRF5, type=testingSet3$userRatingValue)
predDF5

sum5 <- sum(predRF5==testingSet3$userRatingValue) 
length5 <- length(testingSet3$userRatingValue)
accuracy_rfMod5 <- (sum5/length5) 
head(accuracy_rfMod5,30)

```
It turns out the bagImpute with adaptive_cv validation random forest model scored 104/183 correctly as best model with the knnImpute one out bag model.

The next is a medianImpute with adaptive_boot validation.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod6 <- train(userRatingValue ~., method='rf', 
               na.action=na.pass,
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='adaptive_boot'), number=5)
```

```{r}
predRF6 <- predict(rfMod6, testingSet3)

predDF6 <- data.frame(predRF6, type=testingSet3$userRatingValue)
predDF6

sum6 <- sum(predRF6==testingSet3$userRatingValue) 
length6 <- length(testingSet3$userRatingValue)
accuracy_rfMod6 <- (sum6/length6) 
head(accuracy_rfMod6,30)

```
So far the range is 100-104 out of 183 ratings predicted correctly.

The next model is medianImpute with adaptive_cv
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod7 <- train(userRatingValue ~., method='rf', 
               na.action=na.pass, search="random",
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='adaptive_cv'), number=5)
```

```{r}
predRF7 <- predict(rfMod7, testingSet3)

predDF7 <- data.frame(predRF7, type=testingSet3$userRatingValue)
predDF7

sum7 <- sum(predRF7==testingSet3$userRatingValue) 
length7 <- length(testingSet3$userRatingValue)
accuracy_rfMod7 <- (sum7/length7) 
head(accuracy_rfMod7,30)

```
Nothing outside the range yet. Lets keep going.

This next model is medianImpute and adaptive_cv with an added grid search.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod8 <- train(userRatingValue ~., method='rf', 
               na.action=na.pass, search="grid",
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='adaptive_cv'), number=5)
```

```{r}
predRF8 <- predict(rfMod8, testingSet3)

predDF8 <- data.frame(predRF8, type=testingSet3$userRatingValue)
predDF8

sum8 <- sum(predRF8==testingSet3$userRatingValue) 
length8 <- length(testingSet3$userRatingValue)
accuracy_rfMod8 <- (sum8/length8) 
head(accuracy_rfMod8,30)

```
Nothing new on the other model. Lets run two other models of random forest. Lets use the grid search again but use 10 iterations instead of 5 with the medianImputed NAs attribute.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod9 <- train(userRatingValue ~., method='rf', 
               na.action=na.pass, search="grid",
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='adaptive_cv'), number=10)
```

```{r}
predRF9 <- predict(rfMod9, testingSet3)

predDF9 <- data.frame(predRF9, type=testingSet3$userRatingValue)
predDF9

sum9 <- sum(predRF9==testingSet3$userRatingValue) 
length9 <- length(testingSet3$userRatingValue)
accuracy_rfMod9 <- (sum9/length9) 
head(accuracy_rfMod9,30)

```
The above model with medianImpute, 10 iterations of adaptive_cv validation, and grid search did score better than the bottom scoring models at 55 % or 102/183 correct.

This last random forest model uses random search, medianImpute, and 10 iterations of adaptive cv.
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod10 <- train(userRatingValue ~., method='rf', 
               na.action=na.pass, search="random",
               data=(trainingSet3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='adaptive_cv'), number=10)
```

```{r}
predRF10 <- predict(rfMod10, testingSet3)

predDF10 <- data.frame(predRF10, type=testingSet3$userRatingValue)
predDF10

sum10 <- sum(predRF10==testingSet3$userRatingValue) 
length10 <- length(testingSet3$userRatingValue)
accuracy_rfMod10 <- (sum10/length10) 
head(accuracy_rfMod10,30)

```
And the last random forest model scored 54 % with 99/183 correct. This model last ran rfMod10 is actually the lowest scoring model for accuracy in rating predictions.We should also add in the accuracy for our manual mean model that took the difference of the reviews word to all words and compared to all documents in each rating's word to each word of those 12 key words, took the difference, selected the minimum value, and if a tie took the ceiling of the mean of the dot product of the ratings time the votes for each rating and if the ceiling was higher than the highest rating it would select the highest rating. We have this from the Accuracy value variable stored earlier when calculating the correct predicted against total reviews. This was put in the MLr3 table and the code is at 588-589 of this script. The score was 54.397%, which when comparing to these random forest models with varying modifications for validation and NA imputing is in the same range but not the highest, nor the lowest percent.


```{r}
accuracy10RFModels <- as.data.frame(c(accuracy_rfMod1,
                        accuracy_rfMod2, accuracy_rfMod3,
                        accuracy_rfMod4, accuracy_rfMod5,
                        accuracy_rfMod6, accuracy_rfMod7,
                        accuracy_rfMod8, accuracy_rfMod9,
                        accuracy_rfMod10,Accuracy))
colnames(accuracy10RFModels) <- 'accuracyResults'
row.names(accuracy10RFModels) <- c('knnImpute_OOB_5', 
                                   'bagImpute_OOB_5','medianImpute_boot_5',
                                   'knnImpute_boot_5','bagImpute_adaptive_cv_5',
                                  'medianImpute_adaptive_boot_5',
                                  'medianImpute_randomSearch_adaptive_cv_5',
                                  'medianImpute_gridSearch_adaptive_cv_5',
                                  'medianImpute_gridSearch_adaptiv_cv_10',
                                  'medianImpute_randomSearch_adaptive_cv_10',
                                  'manualCeilingMedianDifferenceRatios')

accuracy10RFModels
```


From here I could clean up the text and run the ratios for each and see if other keywords would predict better. Or we could test out the other machine learning models in the caret package any play with their function settings to see if we get better results. We could also use the best model above and test it on the meta data headers extracted from the data. but unable to use for the NAs being dropped with the impute method of NAs.


***




Lets now test out this same set of ratios of the stopwords in predicting our target variable of the rating value.
```{r}
knnMod0 <- train(userRatingValue ~ .,
                method='knn', preProcess=c('center','scale'),
                tuneLength=10, trControl=trainControl(method='adaptive_cv'),
                data=trainingSet3)

```


```{r}
predKNN0 <- predict(knnMod0, testingSet3)
dfKNN0 <- data.frame(predKNN0, true=testingSet3$userRatingValue)
dfKNN0

sumKNN0 <- sum(predKNN0==testingSet3$userRatingValue) 
lengthKNN0 <- length(testingSet3$userRatingValue)
accuracy_knnMod0 <- (sumKNN0/lengthKNN0) 
head(accuracy_knnMod0,30)

```


```{r,eval=FALSE}
rpartMod0 <- train(userRatingValue ~ ., method='rpart', tuneLength=7, data=trainingSet3) 

```

```{r,eval=FALSE}
predRPART0 <- predict(rpartMod0, testingSet3)
dfRPART0 <- data.frame(predRPART0, true=testingSet3$userRatingValue)
dfRPART0

sumRPART0 <- sum(predRPART0==testingSet3$userRatingValue) 
lengthRPART0 <- length(testingSet3$userRatingValue)
accuracy_RPARTMod0 <- (sumRPART0/lengthRPART0) 
head(accuracy_RPARTMod0,30)

```



```{r,eval=FALSE}

RFtunes <- cbind(predDF1[1],predDF2[1],predDF3[1],
             predDF4[1],predDF5[1],predDF6[1],
             predDF7[1],predDF8[1],predDF9[1],
             predDF10[1])
ManualMean <- Reviews15_results$finalPrediction[-inTrain]

predDF11 <- data.frame(RFtunes,ManualMean,dfKNN0[1],dfRPART0[1], 
                      true=testingSet3$userRatingValue)
#the following column name assignment doesn't change the name as intended
colnames(predDF11[12:14]) <-c('predKNN','predRPART','trueValue')

results <- as.data.frame(c(round(accuracy_knnMod0,2), 
             round(accuracy_RPARTMod0,2),
             round(100,2)))
colnames(results) <- 'results'

results$results <- as.factor(paste(results$results))

results1 <- as.data.frame(t(results))
colnames(results1) <- colnames(predDF11[12:14])

acc11 <- as.data.frame(accuracy10RFModels)
colnames(acc11) <- 'results'
acc11$results <- round(acc11$results,2)
acc11$results <- as.factor(paste(acc11$results))
names1 <- colnames(predDF11)[1:10]
row.names(acc11) <- c(names1,'ManualMean')
acc11RFs <- as.data.frame(t(acc11))

resultsAll <- cbind(acc11RFs,results1)
Results <- rbind(predDF11, resultsAll) 
#the column names have to be changed here as well
colnames(Results)[12:13] <- c('predKNN','predRPART')
Results
```


```{r,eval=FALSE}

# The Random Forest package
rfpkg <- randomForest(userRatingValue~., data=trainingSet3, method='class')
predRFpkg <- predict(rfpkg, testingSet3, type='class')
sumRFpkg <- sum(predRFpkg==testingSet3$userRatingValue)
lengthRFpkg <- length(testingSet3$userRatingValue)
accuracy_RFpkg <- sumRFpkg/lengthRFpkg 
# confusionMatrix(predRFpkg, testingSet3$userRatingValue)

# generalizedBoostedModel
gbmMod <- train(userRatingValue~., method='gbm', data=trainingSet3, verbose=FALSE )
predGbm <- predict(gbmMod, testingSet3)
sumGBM0 <- sum(predGbm==testingSet3$userRatingValue)
lengthGBM0 <- length(testingSet3$userRatingValue)
accuracy_gbmMod <- sumGBM0/lengthGBM0 


# linkage dirichlet allocation model
ldaMod <- train(userRatingValue~., method='lda', data=trainingSet3)
predlda <- predict(ldaMod, testingSet3)
sumLDA0 <- sum(predlda==testingSet3$userRatingValue)
lengthLDA0 <- length(testingSet3$userRatingValue)
accuracy_ldaMod <- sumLDA0/lengthLDA0 


CombinedGAM <- train(true~., method='gam', data=predDF11)
CombinedGAMPredictions <- predict(CombinedGAM, predDF11)

predDF12 <- data.frame(predDF11[1:13], predRFpkg, predGbm, predlda,
                       CombinedGAMPredictions,
                     true=testingSet3$userRatingValue)

sumCP <- sum(CombinedGAMPredictions==testingSet3$userRatingValue)
lengthCP <- length(testingSet3$userRatingValue)
accuracy_CP1 <- sumCP/lengthCP

results3 <- as.data.frame(c(accuracy_RFpkg, accuracy_gbmMod, 
                            accuracy_ldaMod, accuracy_CP1, round(100,2)))
colnames(results3) <- 'results'
results3$results <- round(results3$results,2)
results3$results <- as.factor(paste(results3$results))
results4 <- as.data.frame(t(results3))
colnames(results4) <- colnames(predDF12)[14:18]
results5 <- cbind(resultsAll[1:13],results4)
# results3 <- data.frame(t(c(accuracy_RFpkg, accuracy_gbmMod, 
#                            accuracy_ldaMod, accuracy_CP1, round(100,2))))
# 
# 
# names2 <- colnames(predDF12)[14:16]
# colnames(results3) <- c(names2,'CombinedGAMPredictions','true')

accuracyAllResults <- rbind(predDF12,results5)
topbottom <- accuracyAllResults[c(1:10,175:184),]
topbottom
```

The combined model method of 'gam' in the caret package to train the data is supposed to look at all the predicted results and vote on the best value. There is some tuning that needs to be done, or else it is unable to classify more than two classes, hence the 1s and 2s for each prediction. It's accuracy is 10%, the others ranged from 49-57% in accuracy.

Use regression instead of classification for Generalized Linear Machines, temporarily change the target to a numeric instead of factor data type.
```{r, message=FALSE, warning=FALSE, error=FALSE,eval=FALSE}


# glmMod0 <- train(userRatingValue ~ ., metric='Accuracy',
#                 method='glm', data=trainingSet3) 

############################
trainingSet3$userRatingValue <- as.numeric(paste(trainingSet3$userRatingValue))
testingSet3$userRatingValue <- as.numeric(paste(testingSet3$userRatingValue))

glmMod0 <- train(userRatingValue ~ .,
                method='glm', data=trainingSet3)

```
The above isn't letting the glm method for linear classification be used, even with the target turned into a numeric data type and using the metric 'Kappa' or 'Accuracy' for classification. So it will be omitted, it was working with a rounded output of 27% accuracy, but then it stopped working all together when re-ran again. 


```{r}

predGLM0 <- predict(glmMod0, testingSet3) #a numeric vector data type


dfGLM0 <- data.frame(predGLM0,
                       type=testingSet3$userRatingValue)
dfGLM0$predGLM0 <- round(dfGLM0$predGLM0,0)
dfGLM0$predGLM0 <- ifelse(dfGLM0$predGLM0>5,5,dfGLM0$predGLM0)
dfGLM0$predGLM0 <- as.factor(paste(dfGLM0$predGLM0))
head(dfGLM0)
```

```{r}
sumGLM0 <- sum(dfGLM0$predGLM0==testingSet3$userRatingValue)
lengthGLM0 <- length(testingSet3$userRatingValue)
accuracy_GLMMod0 <- (sumGLM0/lengthGLM0)
accuracy_GLMMod0

```
We see that the generalized linear model didn't see a noticeable way the ratios are connected to the outputs, and the accuracy was the worst with 28% accuracy. Worse than any of all the random forest tuned variations of models, the topic modeler Latent Dirichlet Allocation, the generalized boosted machines, the k-nearest neighbor, the recursive partitioned trees, except for the gam voting model that scored 10%. Lets change the target back to a factor data type.
```{r}
testingSet3$userRatingValue <- as.factor(paste(testingSet3$userRatingValue))
trainingSet3$userRatingValue <- as.factor(paste(trainingSet3$userRatingValue))
```


What is left to do from here would be to change out the original filtering of top words and exclude the stopwords in the prepocessing step where we built the document term matrices, and also for the manual program we built to replace all searched words to those 12 words that are top for each rating when all stopwords are not included. This will be a separate file and not an attachment.