---
title: "Sentiment Analysis with Reviews Web Scraped"
author: "Janis Corona"
date: "3/30/2020"
output: html_document
---


This is a web scraped social media review site of two chriopractic clinics offering massage, one low priced grocery store, and one high end massage retreat.The names of the doctors have been replaced with 'DOCTOR', and name of chiropracic facility to CHIROPRACTIC. The names of the high end massage retreat have been replaced with 'HIGH END SPA'. The name of the low cost grocery store was replaced with 'LOW COST GROCERY STORE.'

This data table originally had 1338 observations, but that was an error due to copy and paste in Excel, so there is a need to remove empty rows after reading in the data if your copy of RStudio reads in those empty rows. After reading in the data there will be 516 reviews of mixed ratings for these business models. There aren't many reviews lower than four stars for the chiropractic clinics, but there is a lot of variation in the high end massage retreat and low cost grocery store reviews. These businesses are in the Corona area and the first to be listed when typing massage, except for the grocery store, because it was directly typed in. Note that the social media site will send your api information on your demographics to these businesses when extracting the data, so you should have an alias. 
```{r}
library(DT)
library(tidyverse)
library(dplyr)
library(lubridate)
library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(textstem)
library(stringr)
library(visNetwork)
library(igraph)

```

```{r}
reviews <- read.csv('ReviewsMassageChiropractorYelp_withCompanyNamesOmitted.csv',
                    sep=',',header=TRUE, na.strings=c('',' ','NA','NULL'))
```

Clean up this data of NA rows and empty fields if you have more than 516 observations. You should have five columns.
```{r}
Reviews <- reviews[complete.cases(reviews),]
```


```{r}
colnames(Reviews)
```

You can download this data to follow along with this DT datatable.
```{r}
Reviews_DT <- datatable(data=Reviews, rownames=FALSE,  
                      extensions=c('Buttons','Responsive'),
                      filter=list(position='top'),
                      options=list( dom='Bfrtip',scrollX = TRUE, scrollY=TRUE,
                        buttons=c('colvis','csv'),
                        language=list(sSearch='Filter:')
                        )
                      )
Reviews_DT
```

The rating has more than one rating, separated by a comma for those reviews that are updated and the other reviews displayed have different ratings. We will see this after running the next chunk. The first listed value is the rating for the latest review, and the subsequent ratings (1-5) are for the next subsequent reviews backtracking in time. Each review will have a date listed before each previous review that the later review updated.
```{r}
unique(Reviews$rating_last_first_if_multipleUpdated)
```

You can see from the above there are various review values, and we could choose to keep these or make separate dummy fields for how many time the review was updated from last to first review. The most updates appears to be five times, so we could create dummy fields to capture that rating and if it is the first,second,..., or fifth review and rating for each reviewer. Why not lets just do this. And so the next chunk will add those dummy fields.
```{r}
rating <- strsplit(as.character(paste(Reviews$rating_last_first_if_multipleUpdated)), split=',')

Reviews$mostRecentVisit_rating <- as.character(paste(lapply(rating,'[',1)))
Reviews$lastVisit_rating <- as.character(paste(lapply(rating,'[',2)))
Reviews$twoVisitsPrior_rating <- as.character(paste(lapply(rating,'[',3)))
Reviews$threeVisitsPrior_rating <- as.character(paste(lapply(rating,'[',4)))
Reviews$fourVisitsPrior_rating <- as.character(paste(lapply(rating,'[',5)))

Reviews1 <- Reviews[with(Reviews, order(fourVisitsPrior_rating,
                                        threeVisitsPrior_rating,
                                        twoVisitsPrior_rating,
                                        lastVisit_rating, decreasing=FALSE)),]

head(Reviews1)
```

The order by decreasing=FALSE had to be used to see those sequential visits from last visit, because these fields are character fields. When using predictive analytics they can be changed to factor, or we can change them to numeric.
```{r}
Reviews1$mostRecentVisit_rating <- as.numeric(paste(Reviews1$mostRecentVisit_rating))
Reviews1$lastVisit_rating <- as.numeric(paste(Reviews1$lastVisit_rating))
Reviews1$twoVisitsPrior_rating <- as.numeric(paste(Reviews1$twoVisitsPrior_rating))
Reviews1$threeVisitsPrior_rating <- as.numeric(paste(Reviews1$threeVisitsPrior_rating))
Reviews1$fourVisitsPrior_rating <- as.numeric(paste(Reviews1$fourVisitsPrior_rating))
str(Reviews1)
```

Now our 'NA' filled dummy columns are recognized as actual missing values or NAs of numeric instead of character fields. It is easier to turn the character fields into numeric, then factors so I changed them into numeric. If we want to use them as factors, which they are, when running the models we can. But we are going to focus on extracting hidden features from the data first and cleaning up redundancies in the data from the web scraping extenstions. Like the header user information and the extra dates, or the actual dates, and the 'updated' header to every previous review update. Lets look at our table of Reviews right now, but using the DT package for the datatable function.
```{r}
Reviews_DT1 <- datatable(data=Reviews1, rownames=FALSE, # width = 800, height = 700,
                      extensions=c('Buttons','Responsive'),#'FixedColumns'),
                      #filter=list(position='top'),
                      options=list(pageLength=1,
                        dom='Bfrtip',scrollX = TRUE,# scrollY=TRUE,fixedColumns = TRUE,
                        buttons=c('colvis','csv'),
                        language=list(sSearch='Filter:')
                        )
                      )
                  
Reviews_DT1
```

We should also look at the table within Rmarkdown, because DT is fussy and takes a while to load, plus the amount of text in the first column takes up the rest of the rows.

```{r}
row.names(Reviews1) <- NULL
head(Reviews1)
```
Reviews1 data table is ordered by the review with the most previous reviews in most to least. We see from this first observation in the table and many others that the reviews have a header that needs cleaning up. So, lets do that. We will use gsub to remove these headers with some regex commands.
There are a lot of non character elements in the reviews that are considered white space characters for (\t)tabs, (\n)newlines or a mac newline(\r) or (\s)space. If any mistakes it'll be easy to adjust instead of rerunning codes to get the Reviews1 table.
```{r}
Reviews2 <- Reviews1 

Reviews2$review <- gsub('[P].*[.][\\t][\\n]','',perl=TRUE,Reviews2$review)
head(Reviews2)
```

We see that the Photo... header was removed if the review included a header. That placemarker is removed. But lets also remove the observations that didn't have a photo placemarker and with these observations remove anything between the header and the first listed date that is preceeded by two newlines and one tab.We will also extract the first name and the header of the observations we removed the photo placemarker from up to the first date listed.
```{r}
Reviews2$review <- gsub('^[\\t][\\n]', '',  perl=TRUE, Reviews2$review)

head(Reviews2)

```

I noticed there is a user name that begins with a single apostrophe, and it throws off this script if not fixed early, because later these names will be put into the userName field.So we have to add in the escape character backslash and apostrophe with a pipe for 'or' into this next command.
```{r}
Reviews2$review <- gsub('^[a-zA-Z|\'].*[.]','', perl=TRUE, Reviews2$review)
head(Reviews2)
```

We see that we have the city, state, number of friends, reviews, photos, and a status if they have more than a certain number of reviews. But also that this information could be useful, so we might want to split these string reviews by the 9 newline characters.
```{r}
reviewStringSplit <- strsplit(Reviews2$review, split='[\n]{9}',perl=TRUE)
head(reviewStringSplit)
```
***
This is great, because now we can separate the review with the header information. Lets name one string the headerData and the other the userObservation.
```{r}
headerData <- lapply(reviewStringSplit, '[',1)
head(headerData)
```

***
We see the city, state, number of friends, the number of reviews, number of photos, and if elite separated by a newline. Lets remove the newline, then add all these separately to our table by feature identified accordingly.
```{r}
headerData2 <- as.character(headerData)
headerData2 <- gsub('^[\n]','', headerData2, perl=TRUE)
headermetaSplit <- strsplit(headerData2,split='[\n]',perl=TRUE)
head(headermetaSplit)
```
```{r}
Reviews2$cityState <- lapply(headermetaSplit,'[',1)
Reviews2$friends <- lapply(headermetaSplit,'[',2)
Reviews2$reviews <- lapply(headermetaSplit,'[',3)
Reviews2$photos <- lapply(headermetaSplit,'[',4)
Reviews2$eliteStatus <- lapply(headermetaSplit,'[',5)

```


```{r}
userObservation <- lapply(reviewStringSplit,'[',2)
head(userObservation,3)
```
***
We can see from the second split of the string that the userObservation includes more meta data that includes the user name, date, if it was an updated review, and if available the number of check ins to the business and number of photos. The first name is the first part of this string. Lets split the string and make a field called userName to add to our table. We have to split by one newline and one tab, because one of the dates only has a prepend of one newline and one tab, although most are two newlines followed by a tab, we miss a review if we don't.
```{r}
obsStrSplit <- as.character(userObservation)
obsStrSplit2 <- strsplit(obsStrSplit,split='[\n][\t]', perl=TRUE)
Reviews2$userName <- lapply(obsStrSplit2,'[',1)
Reviews2$userName <- gsub('[\n][\n]','',perl=TRUE, Reviews2$userName)
Reviews2$userName <- gsub('^[ ]','',perl=TRUE, Reviews2$userName)
Reviews2$userName <- gsub('[\n]$','',perl=TRUE, Reviews2$userName)
head(Reviews2$userName)
```

Lets now replace the review field that has been slightly adjusted to exclude the first header. There is still the user name header that occurs right before the data and if the review is updated.This is the obsStrSplit2 list.
```{r}
head(obsStrSplit2)
```
***

These top reviews are the ones that have more than one review ordered from most to least by rating when this analysis and data cleaning began. We see from our obsStrSplit2 object that there are at most 5 listed reviews that were separated by the double newline and tab that preceeds each date. So we can make dummy fields for these reviews as well by the listed item they correspond to for each user. Later we can gather those fields into one Review field by review by visit. In each of those fields we will take out the response by the business if there was one. They are shown above and start with a double newline and the words, 'Business Customer Service.' We'll mark this so we know to search for this fix later. %^& its tagged.
So, lets add in the latest review, previous review, 2nd previous review, 3rd previous review, and 4th previous review because the most reviews for this data is five.
```{r}
Reviews2$mostRecentVisit_review <- as.character(paste(lapply(obsStrSplit2,'[',2)))
Reviews2$lastVisit_review <- as.character(paste(lapply(obsStrSplit2,'[',3)))
Reviews2$twoVisitsPrior_review <- as.character(paste(lapply(obsStrSplit2,'[',4)))
Reviews2$threeVisitsPrior_review <- as.character(paste(lapply(obsStrSplit2,'[',5)))
Reviews2$fourVisitsPrior_review <- as.character(paste(lapply(obsStrSplit2,'[',5)))
```

lets remove the first review field from our table using the dplyr package select function. We should also change these added list types so that they are character strings.
```{r}
Reviews3 <- Reviews2 %>% select(-review)
Reviews3$cityState <- as.factor(paste(Reviews3$cityState))
Reviews3$friends <- gsub(' friends','',Reviews3$friends)
Reviews3$friends <- as.numeric(paste(Reviews3$friends))
Reviews3$reviews <- gsub(' reviews','', Reviews3$reviews)
Reviews3$reviews <- as.numeric(paste(Reviews3$reviews))
Reviews3$photos <- gsub(' photos','', Reviews3$photos)
Reviews3$photos <- as.numeric(paste(Reviews3$photos))
Reviews3$eliteStatus <- as.factor(paste(Reviews3$eliteStatus))
Reviews3$mostRecentVisit_review <- as.character(Reviews3$mostRecentVisit_review)
Reviews3$lastVisit_review <- as.character(Reviews3$lastVisit_review)
Reviews3$twoVisitsPrior_review <- as.character(Reviews3$twoVisitsPrior_review)
Reviews3$threeVisitsPrior_review <- as.character(Reviews3$threeVisitsPrior_review)
Reviews3$fourVisitsPrior_review <- as.character(Reviews3$fourVisitsPrior_review)
str(Reviews3)
```

Lets rearrange the columns in our new table. We still need to extract from each of the five reviews by user (if exist) the business response (also if exists).
```{r}
colnames(Reviews3)
```

First lets gather the review fields and the ratings fields and remove the NA values from the userReveiwSeries and userRatingSeries.
```{r}
Reviews4 <- gather(Reviews3, 'userReviewSeries','userReviewContent',16:20)
Reviews4$userReviewContent <- gsub('NA','', Reviews4$userReviewContent)

#remove the char NAs because complete.cases won't work unless the table is read in with
# the correct NA values, even after converting to empty in the table.
write.csv(Reviews4, 'reviews4.csv', row.names=FALSE)
Reviews4 <- read.csv('reviews4.csv', sep=',', header=TRUE, na.strings=c('',' ','NA',NULL))

#now the table is 543 instead of 2580 observations.
Reviews4 <- Reviews4[complete.cases(Reviews4$userReviewContent),]

Reviews5 <- gather(Reviews4, 'userRatingSeries','userRatingValue',5:9)
#because this userRatingValue field is numeric, the NAs are already read by R as such
#we can remove the NAs with complete.cases to get a 614 obs table instead of 2715
Reviews5 <- Reviews5[complete.cases(Reviews5$userRatingValue),]

colnames(Reviews5)

```


```{r}
Reviews6 <- Reviews5 %>% select(userReviewSeries, userReviewContent,
                                userRatingSeries, userRatingValue,
                                everything())
Reviews7 <- Reviews6 %>% select(-rating_last_first_if_multipleUpdated,
                                -site)
```



```{r}
head(Reviews7)
```

Lets now remove the business response from the review content field.
```{r}
businessReplied <- grep('Comment from',Reviews7$userReviewContent)
Reviews7$businessReplied <- 'no'
Reviews7$businessReplied[businessReplied] <- 'yes'

Reviews8 <- Reviews7 %>% select(userReviewSeries:userRatingValue,businessReplied,
                                everything())
Reviews9 <- Reviews8[order(Reviews8$businessReplied, decreasing=TRUE),]
row.names(Reviews9) <- NULL
```

Lets make this field a new field of the public relations reply removed.
```{r}
Reviews9$userReviewContent <- as.character(paste(Reviews9$userReviewContent))
Reviews9$userRatingSeries <- as.factor(paste(Reviews9$userRatingSeries))
Reviews9$businessReplied <- as.factor(Reviews9$businessReplied)

PR <- strsplit(Reviews9$userReviewContent, split='[C][o][m][m][e][n][t] [f][r][o][m]',
               perl=TRUE)
head(PR)
```

Lets separate these into two separate character strings of user only review and PR_reply
```{r}
userOnlyReview <- as.character(paste(lapply(PR,'[',1)))
PR_reply <- as.character(paste(lapply(PR,'[',2)))
```

Both of the above vectors are the same number of observations as our table. 

Lets remove the other data on photos
```{r}
userOnlyReview <- gsub('[\n][P][h][o][t][o] [o][f].*','', userOnlyReview,perl=TRUE)
grep('Comment', userOnlyReview)
```
There shouldn't be any comments from business owners in this first part of the string.

```{r}
head(userOnlyReview,6)
```

***
Also, the above shows the beginning is a date with a dropped zero for the months 1-9, and some observations have the photo[s] or check-in[s]. This should be modified with regex to add a date column and also add the numeric values for the photos or check-ins to those fields in our table.


Lets add these two strings of user only and PR reply to the data as two separate fields with ifelse functions.
```{r}
Reviews9$userReviewOnlyContent <- userOnlyReview

Reviews9$businessReplyContent <- PR_reply

Reviews9$userReviewOnlyContent <- gsub('[\n][P][h][o][t][o] [o][f].*','', 
                                   Reviews9$userReviewOnlyContent,perl=TRUE)
Reviews9$userReviewOnlyContent <- gsub('[S][e][e] [a][l][l] [p].*','',
                                       Reviews9$userReviewOnlyContent,
                                   perl=TRUE)
head(Reviews9[,13:15])
```

We just mentioned the date beginning each userReviewOnlyContent and userReviewContent columns, so lets create a date column for these dates. There are actually a bunch of anomolies in that first part of the string.
```{r}
Reviews9$Date <- substr(Reviews9$userReviewOnlyContent,1,11)
date <- strsplit(Reviews9$Date, split='[a-zA-Z]', perl=TRUE)
Date <- as.character(paste(lapply(date,'[',1)))
Date <- trimws(Date, which='right',whitespace='[\n]')
Date <- gsub('[ ][\n][0-9]','', perl=TRUE, Date)
Date <- gsub('[\n][ ][0-9]','', perl=TRUE, Date)
Date <- gsub('[\n][ ]','', perl=TRUE, Date)
Date <- gsub('[\n][\" ][0-9]','', perl=TRUE, Date)
Date <- gsub('[\n][0-9]{2}','', perl=TRUE, Date)
Date <- gsub('[\n][\\]["]','', perl=TRUE, Date)
Date <- gsub('[\n][0-9]','', perl=TRUE,Date)

Date1 <- mdy(Date)

Reviews9$Date <-Date1

```

Remove the first date string in the userReviewOnlyContent.
```{r}
Reviews9$userReviewOnlyContent <- gsub('[0-9]{1,2}[/][0-9]{1,2}[/][0-9]{4}','',
                                       perl=TRUE, Reviews9$userReviewOnlyContent)
```

Lets also remove any photo meta from the original userReviewContent column.
```{r}
Reviews9$userReviewContent <- gsub('[S][e][e] [a][l][l] [p].*','',Reviews9$userReviewContent,
                                   perl=TRUE)
Reviews9$userReviewContent <- gsub('[\n][P][h][o][t][o] [o][f].*','',
                                   Reviews9$userReviewContent,perl=TRUE)
```

Now lets rearrange our columns and make this a searchable and downloadable datatable.
```{r}
Reviews10 <- Reviews9 %>% select(userReviewSeries, userReviewOnlyContent,
                                 userRatingSeries, userRatingValue, businessReplied,                                                businessReplyContent, everything())
colnames(Reviews10)
```

The userReviewContent has the text of both business response and the user as well as photo placemarker data.
```{r}
Reviews10_DT <- datatable(data=Reviews10, rownames=FALSE, # width = 800, height = 700,
                      extensions=c('Buttons','Responsive'),#'FixedColumns'),
                      #filter=list(position='top'),
                      options=list(pageLength=1,
                        dom='Bfrtip',scrollX = TRUE,# scrollY=TRUE,fixedColumns = TRUE,
                        buttons=c('colvis','csv'),
                        language=list(sSearch='Filter:')
                        )
                      )
Reviews10_DT
```


```{r}
head(Reviews10)
```

The userReviewContent was kept in the table to compare the cleaned up columns on user reviews.

We should still remove the updated and previous review descriptions.
```{r}
Reviews10$userReviewOnlyContent <- gsub('[uU][p][d][a][t][e][d].*[\n]','', perl=TRUE,
                                        Reviews10$userReviewOnlyContent)
Reviews10$userReviewOnlyContent <- gsub('[pP][r][e][v][i][o][u].*[w]','', perl=TRUE,
                                        Reviews10$userReviewOnlyContent)
Reviews10$id <- row.names(Reviews10)

pix <- grep('photo+',Reviews10$userReviewOnlyContent)
pix2 <- Reviews10$userReviewOnlyContent[pix]
pix3 <- trimws(pix2, which="left",whitespace="[\t\r\n]")
pixs <- as.data.frame(pix3)
colnames(pixs) <- 'busPhotos'
pixs$id <- pix
pixs$busPhotos <- gsub('^[ ]','', pixs$busPhotos)
pixs2 <- pixs[grep('^[0-9][ ][pP]',pixs$busPhotos, perl=TRUE),]
head(pixs2)
```

```{r}
pics <- strsplit(pixs2$busPhotos,split='[\n\n]',perl=TRUE)
head(pics,2)
```

From the above, we our only interested in the first split of photos, the other reviews are split on the double newline and caused multiple splits for most single reviews.
```{r}
pixs2$userBusinessPhotos <- as.character(paste(lapply(pics,'[',1)))
pixs2$userBusinessPhotos <- gsub(' photo','', pixs2$userBusinessPhotos)
pixs2$userBusinessPhotos <- gsub('s','',pixs2$userBusinessPhotos)
pixs2$userBusinessPhotos <- trimws(pixs2$userBusinessPhotos,which='right')
pixs2$userBusinessPhotos <- as.numeric(paste(pixs2$userBusinessPhotos))
head(pixs2)
```

Lets keep only the id and userBusinessPhotos columns.
```{r}
pics3 <- pixs2 %>% select(id,userBusinessPhotos)
```

Combine this new feature to the data table of all features thus far.
```{r}
Reviews11 <- merge(Reviews10, pics3, by.x='id', by.y='id', all.x=TRUE)
```

Now lets do the same thing for the check-ins information. The number of times the user checked in or visited the business. Get only those reviews
with the check-ins a header and not in the observation or found at the end of the observation.
```{r}
checks <- Reviews11 %>% select(id,userReviewOnlyContent) 
  
checks$substring <- substr(checks$userReviewOnlyContent, 1,40) 
  
chekn <- grep('check-in',checks$substring)
checks1 <- checks[chekn,]
```
There are more check-ins than photos by the user. 
```{r}
chekn <- checks1 %>% select(id, substring)
head(chekn,10)
```

Lets remove the reference to photos and double newline characters from our substring.
```{r}
chekn$substring <- gsub('[0-9] [p][h][o][t][o][\n][\n]','', chekn$substring,perl=TRUE)
chekn$substring <- gsub('[0-9][0-9] [p][h][o][t][o][s][\n][\n]','', chekn$substring,perl=TRUE)
chekn$substring <- gsub('[0-9] [p][h][o][t][o][s][\n][\n]','', chekn$substring,perl=TRUE)

```

Split on the double newline characters and grab the first entries, after verifying the substring column only starts with the number of check-ins per user.
```{r}
checkN <- strsplit(chekn$substring, split='[\n][\n]',perl=TRUE)
head(checkN)
```

```{r}
checkN2 <- as.character(paste(lapply(checkN,'[',1)))
checkN2 <- trimws(checkN2, which='left', whitespace="[\n]")
checkN2 <- gsub('^ ','',checkN2)
checkN2 <- gsub('^ ','',checkN2)
checkN2 <- gsub(' check-ins','',checkN2)
checkN2 <- gsub(' check-in','',checkN2)
checkN2 <- as.numeric(paste(checkN2))

head(checkN2)
```

```{r}
chekn$userCheckIns <- checkN2
head(chekn)
```

merge this with Reviews11 data.
```{r}
Reviews12 <- merge(Reviews11, chekn, by.x='id', by.y='id', all.x=TRUE)
head(Reviews12[order(Reviews12$userCheckIns,decreasing=TRUE),c(17,19:20)])
```

```{r}
Reviews13 <- Reviews12 %>% select(-id, -substring)
head(Reviews13[order(Reviews13$userCheckIns,decreasing=TRUE),c(16,18)])
```

Lets remove the header from the userReviewOnlyContent column now that we have extracted the photos and check-in data per user.
```{r}
subUser <- substr(Reviews13$userReviewOnlyContent,1,30)
head(subUser,30)

```

```{r}
subUser2 <- gsub('[0-9]{1,2}.*[\n][\n]','', subUser, perl=TRUE)
head(subUser2,30)

```

```{r}
subUser3 <- gsub('[\n][ ][0-9]{1,2}.*[\n][\n]','',subUser2, perl=TRUE)
head(subUser3,50)
```

Now that we tested the removal using regex on a string, we can apply these regex commands to the column userReviewOnlyContent.
```{r}
Reviews13$userReviewOnlyContent <- gsub('[0-9]{1,2}.*[\n][\n]','',
                                        Reviews13$userReviewOnlyContent, perl=TRUE)

Reviews13$userReviewOnlyContent <- gsub('[\n][ ][0-9]{1,2}.*[\n][\n]','',
                                        Reviews13$userReviewOnlyContent, perl=TRUE)
Reviews13$userReviewOnlyContent <- trimws(Reviews13$userReviewOnlyContent, which='left',
                                          whitespace="[\n\t\r]")
head(Reviews13,30)
```
Now it looks like we have our cleaned data to run sentiment and text analysis of in determining the rating the user review is given by the user. Lets write this file out to csv, and make a DT datatable for downloading from Rpubs.
```{r}
write.csv(Reviews13, 'cleanedRegexReviews13.csv',row.names=FALSE)
```


```{r}
Reviews13_DT <- datatable(data=Reviews13, rownames=FALSE,  #width = 800, height = 700,
                      extensions=c('Buttons','Responsive'),#,'FixedColumns'),
                      filter=list(position='top'),
                      options=list(pageLength=2,
                        dom='Bfrtip',scrollX = TRUE, scrollY=TRUE,#fixedColumns = TRUE,
                        buttons=c('colvis','csv'),
                        language=list(sSearch='Filter:')
                        )
                      )
Reviews13_DT
```

Lets keep only the cleaned user review and the rating for that user's visit.
```{r}
Reviews14 <- Reviews13 %>% select(userReviewOnlyContent,userRatingValue)
row.names(Reviews14) <- NULL
head(Reviews14)
```

We will have to create a corpus of documents for each rating, then we can clean up the text with the programs within these text mining libraries other than what we have done to the data already. We should also remove the words: 'DOCTOR', 'CHIROPRACTIC,'HIGH END SPA', and 'LOW COST GROCERY STORE.'
```{r}
Reviews14$userReviewOnlyContent <- gsub('DOCTOR','', Reviews14$userReviewOnlyContent)
Reviews14$userReviewOnlyContent <- gsub('CHIROPRACTIC','', Reviews14$userReviewOnlyContent)
Reviews14$userReviewOnlyContent <- gsub('HIGH END SPA','', Reviews14$userReviewOnlyContent)
Reviews14$userReviewOnlyContent <- gsub('LOW COST GROCERY STORE','',
                                        Reviews14$userReviewOnlyContent)

```

Lets lemmatize the document first to grab the root word and not the stem of each review.
```{r}
lemma <- lemmatize_strings(Reviews14$userReviewOnlyContent, dictionary=lexicon::hash_lemmas)

Lemma <- as.data.frame(lemma)
Lemma <- cbind(Lemma, Reviews14)

colnames(Lemma) <- c('lemmatizedReview','review', 'rating')
Lemma$rating <- as.factor(paste(Lemma$rating))
head(Lemma)
```

From this table, we are going to subset the reviews by rating by the user of 1 through 5.
```{r}
rating1 <- subset(Lemma, Lemma$rating==1)
rating2 <- subset(Lemma, Lemma$rating==2)
rating3 <- subset(Lemma, Lemma$rating==3)
rating4 <- subset(Lemma, Lemma$rating==4)
rating5 <- subset(Lemma, Lemma$rating==5)

```

Lets create a directory for each rating.Erase the eval=FALsE, if you want to run this script. I already have the files.
```{r, error=FALSE, message=FALSE, warning=FALSE,eval=FALSE}
dir.create('./rating1')
dir.create('./rating2')
dir.create('./rating3')
dir.create('./rating4')
dir.create('./rating5')

r1 <- as.character(rating1$lemmatizedReview)
setwd('./rating1')

for (j in 1:length(r1)){
  write(r1[j], paste(paste('rating1',j, sep='.'), '.txt', sep=''))
}
setwd('../')

r2 <- as.character(rating2$lemmatizedReview)
setwd('./rating2')

for (j in 1:length(r2)){
  write(r2[j], paste(paste('rating2',j, sep='.'), '.txt', sep=''))
}
setwd('../')


r3 <- as.character(rating3$lemmatizedReview)
setwd('./rating3')

for (j in 1:length(r3)){
  write(r3[j], paste(paste('rating3',j, sep='.'), '.txt', sep=''))
}
setwd('../')

r4 <- as.character(rating4$lemmatizedReview)
setwd('./rating4')

for (j in 1:length(r4)){
  write(r4[j], paste(paste('rating4',j, sep='.'), '.txt', sep=''))
}
setwd('../')


r5 <- as.character(rating5$lemmatizedReview)
setwd('./rating5')

for (j in 1:length(r5)){
  write(r5[j], paste(paste('rating5',j, sep='.'), '.txt', sep=''))
}
setwd('../')

```

List the files in each folder rating 1-5.
```{r}
list.files('./rating1')
```



```{r}
list.files('./rating2')
```

```{r}
list.files('./rating3')

```

```{r}
list.files('./rating4')

```

```{r}
list.files('./rating5')

```


```{r}
R1 <- Corpus(DirSource("rating1"))


R1

R1 <- tm_map(R1, removePunctuation)
R1 <- tm_map(R1, removeNumbers)
#R1 <- tm_map(R1, tolower) # I want to capture the emotion the users write with when All caps
#R1 <- tm_map(R1, removeWords, stopwords("english"))#Also the number of 'and's' and 'not' etc
R1 <- tm_map(R1, stripWhitespace)
#+R1 <- tm_map(R1, stemDocument)#we already lemmatized the document it is more robust to meaning

dtmR1 <- DocumentTermMatrix(R1)
```

```{r}
freq <- colSums(as.matrix(dtmR1))
wordcloud(names(freq), freq, min.freq=30,colors=brewer.pal(3,'Dark2'))

```

```{r}
freqR1 <- as.data.frame(colSums(as.matrix(dtmR1)))
colnames(freqR1) <- 'rating1'
freqR1$id <- row.names(freqR1)
FREQ_R1 <- freqR1[order(freqR1$rating1,decreasing=TRUE),]
row.names(FREQ_R1) <- NULL
head(FREQ_R1,50)
```

People in general, speaking as American born and raised, when angry usually speak out of anger and disappointment when feeling they have been had, taken, or in some way been victimized for not getting what they paid for when promised or convinced into getting that experience, purchase, feeling, etc. As you can see by not excluding the stop words we now have a count of the number of connections to persuade the reader that he or she was wronged or rewarded by the number of interjections. We learn this early in persuasive writing in grammar school to have at least five paragraphs to build a persuasive story with an introduction, three body paragraphs, and a conclusion, and again in English composition in lower level undergrad work. This means build on three points or perspectives in the body paragraphs to persuade the reader your right, and find them if they aren't readily considered.

Lets do the same for the other four folders in getting our ordered word counts or frequencies.
```{r}
R2 <- Corpus(DirSource("rating2"))


R2

R2 <- tm_map(R2, removePunctuation)
R2 <- tm_map(R2, removeNumbers)
R2 <- tm_map(R2, stripWhitespace)

dtmR2 <- DocumentTermMatrix(R2)

freq2 <- colSums(as.matrix(dtmR2))
wordcloud(names(freq2), freq2, min.freq=25,colors=brewer.pal(3,'Dark2'))
```

```{r}
freqR2 <- as.data.frame(colSums(as.matrix(dtmR2)))
colnames(freqR2) <- 'rating2'
freqR2$id <- row.names(freqR2)
FREQ_R2 <- freqR2[order(freqR2$rating2,decreasing=TRUE),]
row.names(FREQ_R2) <- NULL
head(FREQ_R2,50)
```

```{r}
R3 <- Corpus(DirSource("rating3"))


R3

R3 <- tm_map(R3, removePunctuation)
R3 <- tm_map(R3, removeNumbers)
R3 <- tm_map(R3, stripWhitespace)

dtmR3 <- DocumentTermMatrix(R3)

freq3 <- colSums(as.matrix(dtmR3))
wordcloud(names(freq3), freq3, min.freq=25,colors=brewer.pal(3,'Dark2'))
```

```{r}
freqR3 <- as.data.frame(colSums(as.matrix(dtmR3)))
colnames(freqR3) <- 'rating3'
freqR3$id <- row.names(freqR3)
FREQ_R3 <- freqR3[order(freqR3$rating3,decreasing=TRUE),]
row.names(FREQ_R3) <- NULL
head(FREQ_R3,50)
```

```{r}
R4 <- Corpus(DirSource("rating4"))


R4

R4 <- tm_map(R4, removePunctuation)
R4 <- tm_map(R4, removeNumbers)
R4 <- tm_map(R4, stripWhitespace)

dtmR4 <- DocumentTermMatrix(R4)

freq4 <- colSums(as.matrix(dtmR4))
wordcloud(names(freq4), freq4, min.freq=25,colors=brewer.pal(3,'Dark2'))
```

```{r}
freqR4 <- as.data.frame(colSums(as.matrix(dtmR4)))
colnames(freqR4) <- 'rating4'
freqR4$id <- row.names(freqR4)
FREQ_R4 <- freqR4[order(freqR4$rating4,decreasing=TRUE),]
row.names(FREQ_R4) <- NULL
head(FREQ_R4,50)
```

```{r}
R5 <- Corpus(DirSource("rating5"))


R5

R5 <- tm_map(R5, removePunctuation)
R5 <- tm_map(R5, removeNumbers)
R5 <- tm_map(R5, stripWhitespace)

dtmR5 <- DocumentTermMatrix(R5)

freq5 <- colSums(as.matrix(dtmR5))

wordcloud(names(freq5), freq5, min.freq=75,colors=brewer.pal(3,'Dark2'))
```

```{r}
freqR5 <- as.data.frame(colSums(as.matrix(dtmR5)))
colnames(freqR5) <- 'rating5'
freqR5$id <- row.names(freqR5)
FREQ_R5 <- freqR5[order(freqR5$rating5,decreasing=TRUE),]
row.names(FREQ_R5) <- NULL
head(FREQ_R5,50)
```

Lets add a feature for the ratio of word frequencies to the number of documents in the reviews with each rating 1-5.
```{r}
l1 <- length(list.files('./rating1'))
l2 <- length(list.files('./rating2'))
l3 <- length(list.files('./rating3'))
l4 <- length(list.files('./rating4'))
l5 <- length(list.files('./rating5'))

FREQ_R1$termTotalFilesRatio <-FREQ_R1$rating1/l1
FREQ_R2$termTotalFilesRatio <- FREQ_R2$rating2/l2
FREQ_R3$termTotalFilesRatio <- FREQ_R3$rating3/l3
FREQ_R4$termTotalFilesRatio <- FREQ_R4$rating4/l4
FREQ_R5$termTotalFilesRatio <- FREQ_R5$rating5/l5

FREQ_R1$termTotalTermsRatio <-FREQ_R1$rating1/length(FREQ_R1$id)
FREQ_R2$termTotalTermsRatio <- FREQ_R2$rating2/length(FREQ_R2$id)
FREQ_R3$termTotalTermsRatio <- FREQ_R3$rating3/length(FREQ_R3$id)
FREQ_R4$termTotalTermsRatio <- FREQ_R4$rating4/length(FREQ_R4$id)
FREQ_R5$termTotalTermsRatio <- FREQ_R5$rating5/length(FREQ_R5$id)

```

Lets change the column names of each rating table.
```{r}
colnames(FREQ_R1) <-c('Rating1termfrequency',
                      'term',
                      'Rating1_termTotalFilesRatio',
                      'Rating1_termTotalTermsRatio')
colnames(FREQ_R2) <-c('Rating2termfrequency',
                      'term',
                      'Rating2_termTotalFilesRatio',
                      'Rating2_termTotalTermsRatio')
colnames(FREQ_R3) <-c('Rating3termfrequency',
                      'term',
                      'Rating3_termTotalFilesRatio',
                      'Rating3_termTotalTermsRatio')
colnames(FREQ_R4) <-c('Rating4termfrequency',
                      'term',
                      'Rating4_termTotalFilesRatio',
                      'Rating4_termTotalTermsRatio')
colnames(FREQ_R5) <-c('Rating5termfrequency',
                      'term',
                      'Rating5_termTotalFilesRatio',
                      'Rating5_termTotalTermsRatio')

```

Lets now combine all these term frequencies.
```{r}
m1 <- merge(FREQ_R1,FREQ_R2, by.x='term', by.y='term', all=TRUE)
m2 <- merge(m1,FREQ_R3, by.x='term', by.y='term', all=TRUE)
m3 <- merge(m2,FREQ_R4, by.x='term', by.y='term', all=TRUE)
m4 <- merge(m3,FREQ_R5, by.x='term', by.y='term', all=TRUE)

allTerms <- m4 %>% select(term,Rating1termfrequency,Rating2termfrequency,
                          Rating3termfrequency,Rating4termfrequency,
                          Rating5termfrequency,Rating1_termTotalFilesRatio,
                          Rating2_termTotalFilesRatio,Rating3_termTotalFilesRatio,
                          Rating4_termTotalFilesRatio,Rating5_termTotalFilesRatio,
                          everything())
colnames(allTerms)
```


Lets add a median field for the word in each rating to this table.
```{r}
allTerms$MedianCount <- apply(allTerms[2:6],1,median, na.rm=TRUE)

medianRating1 <- apply(allTerms[2],2,median,na.rm=TRUE)
medianRating2 <- apply(allTerms[3],2,median,na.rm=TRUE)
medianRating3 <- apply(allTerms[4],2,median,na.rm=TRUE)
medianRating4 <- apply(allTerms[5],2,median,na.rm=TRUE)
medianRating5 <- apply(allTerms[6],2,median,na.rm=TRUE)

meanRating1 <- floor(apply(allTerms[2],2,mean,na.rm=TRUE))
meanRating2 <- floor(apply(allTerms[3],2,mean,na.rm=TRUE))
meanRating3 <- floor(apply(allTerms[4],2,mean,na.rm=TRUE))
meanRating4 <- floor(apply(allTerms[5],2,mean,na.rm=TRUE))
meanRating5 <- floor(apply(allTerms[6],2,mean,na.rm=TRUE))

allTerms2 <- allTerms[order(allTerms$MedianCount,decreasing=TRUE),]
```

Lets add a bottom and top percentile to this table based on the terms in each rating subset.
```{r}
allTerms2$Quantile5_R1 <- ifelse(allTerms2$Rating1termfrequency <=       
                                quantile(allTerms2$Rating1termfrequency, .05,na.rm=TRUE),
                              1,0)
allTerms2$Quantile95_R1 <- ifelse(allTerms2$Rating1termfrequency >=       
                                quantile(allTerms2$Rating1termfrequency, .95,na.rm=TRUE),
                              1,0)
allTerms2$Quantile5_R2 <- ifelse(allTerms2$Rating2termfrequency <=       
                                quantile(allTerms2$Rating2termfrequency, .05,na.rm=TRUE),
                              1,0)
allTerms2$Quantile95_R2 <- ifelse(allTerms2$Rating2termfrequency >=       
                                quantile(allTerms2$Rating2termfrequency, .95,na.rm=TRUE),
                              1,0)
allTerms2$Quantile5_R3 <- ifelse(allTerms2$Rating3termfrequency <=       
                                quantile(allTerms2$Rating3termfrequency, .05,na.rm=TRUE),
                              1,0)
allTerms2$Quantile95_R3 <- ifelse(allTerms2$Rating3termfrequency >=       
                                quantile(allTerms2$Rating3termfrequency, .95,na.rm=TRUE),
                              1,0)
allTerms2$Quantile5_R4 <- ifelse(allTerms2$Rating4termfrequency <=       
                                quantile(allTerms2$Rating4termfrequency, .05,na.rm=TRUE),
                              1,0)
allTerms2$Quantile95_R4 <- ifelse(allTerms2$Rating4termfrequency >=       
                                quantile(allTerms2$Rating4termfrequency, .95,na.rm=TRUE),
                              1,0)
allTerms2$Quantile5_R5 <- ifelse(allTerms2$Rating5termfrequency <=       
                                quantile(allTerms2$Rating5termfrequency, .05,na.rm=TRUE),
                              1,0)
allTerms2$Quantile95_R5 <- ifelse(allTerms2$Rating5termfrequency >=       
                                quantile(allTerms2$Rating5termfrequency, .95,na.rm=TRUE),
                              1,0)

```

We have to keep this data wide, but it is useful to filter by, and extracting those words more used in each rating for each review. 
```{r}
goodGreat <- subset(allTerms2, allTerms2$Quantile95_R5==1 &
                      allTerms2$Quantile95_R4==1 & 
                      allTerms2$Rating5termfrequency > allTerms2$MedianCount &
                      allTerms2$Rating4termfrequency > allTerms2$MedianCount |
                      allTerms2$Quantile5_R5==1 &
                      allTerms2$Quantile5_R4==1 |
                      allTerms2$Rating5termfrequency > meanRating5 |
                      allTerms2$Rating4termfrequency > meanRating4
                  )
average <- subset(allTerms2, allTerms2$Quantile95_R3==1 &
                      allTerms2$Quantile95_R2==1 & 
                      allTerms2$Rating3termfrequency > allTerms2$MedianCount &
                      allTerms2$Rating2termfrequency > allTerms2$MedianCount |
                      allTerms2$Quantile5_R3==1 &
                      allTerms2$Quantile5_R2==1  |
                      allTerms2$Rating3termfrequency > meanRating3 |
                      allTerms2$Rating2termfrequency > meanRating2
                 )
poor <- subset(allTerms2, allTerms2$Quantile95_R1==1 &
                      allTerms2$Quantile95_R2==1 & 
                      allTerms2$Rating1termfrequency > allTerms2$MedianCount &
                      allTerms2$Rating2termfrequency > allTerms2$MedianCount |
                      allTerms2$Quantile5_R1==1 &
                      allTerms2$Quantile5_R2==1 |
                      allTerms2$Rating1termfrequency > meanRating1 |
                      allTerms2$Rating2termfrequency > meanRating2
                 )
```

Here is bar chart of the word counts for the poor ratings.
```{r}
wf <- data.frame(word=poor$term, freq=poor$Rating1termfrequency)
p <- ggplot(subset(wf, freq>60), aes(word, freq))
p <- p + geom_bar(stat= 'identity') 
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1)) 
p
```


Lets make a word cloud of each of these data tables terms by weights of the lowest for poor, highest for average, and highest for goodGreat
The NAs have to be removed before using word cloud.
```{r}
poorNA <- poor[complete.cases(poor$Rating1termfrequency),]

Poor1 <- as.data.frame(t(poorNA$Rating1termfrequency))
colnames(Poor1) <- poorNA$term

Poor1 <- Poor1 %>% select(-and,-the)

freqPoor <- colSums(as.matrix(Poor1))
```

```{r}
wordcloud(names(freqPoor), freqPoor, min.freq=20,
          colors=brewer.pal(6,'Dark2'))

```

```{r}
wordcloud(names(freqPoor), freqPoor, min.freq=25,colors=brewer.pal(3,'Dark2'))

```


Here is bar chart of the word counts for the average ratings.
```{r}
wf <- data.frame(word=average$term, freq=average$Rating3termfrequency)
p <- ggplot(subset(wf, freq>25), aes(word, freq))
p <- p + geom_bar(stat= 'identity') 
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1)) 
p
```


Lets make a word cloud.
```{r}
avgNA <- average[complete.cases(average$Rating3termfrequency),]

Avg1 <- as.data.frame(t(avgNA$Rating3termfrequency))
colnames(Avg1) <- avgNA$term
Avg1 <- Avg1 %>% select(-and,-the)
freqAvg <- colSums(as.matrix(Avg1))
```

```{r}
wordcloud(names(freqAvg), freqAvg, min.freq=20,
          colors=brewer.pal(6,'Dark2'))

```

```{r}
wordcloud(names(freqAvg), freqAvg, min.freq=25,colors=brewer.pal(3,'Dark2'))

```


Here is bar chart of the word counts for the good or great ratings.
```{r}
wf <- data.frame(word=goodGreat$term, freq=goodGreat$Rating5termfrequency)
p <- ggplot(subset(wf, freq>70), aes(word, freq))
p <- p + geom_bar(stat= 'identity') 
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1)) 
p
```


Lets make a word cloud.
```{r}
grtNA <- goodGreat[complete.cases(goodGreat$Rating5termfrequency),]

Grt1 <- as.data.frame(t(grtNA$Rating5termfrequency))
colnames(Grt1) <- grtNA$term
Grt1 <- Grt1[,-c(1:4)] #remove the first 4 words, (the,and,for,have)

freqGrt <- colSums(as.matrix(Grt1))
```

```{r}
wordcloud(names(freqGrt), freqGrt, min.freq=80,
          colors=brewer.pal(6,'Dark2'))

```

```{r}
wordcloud(names(freqGrt), freqGrt, min.freq=95,colors=brewer.pal(3,'Dark2'))

```


***

That was a great way to look at the word clouds of these ratings and the words in each set of words in the top and bottom 5th percentiles as well as higher than the median or mean values.
The last couple of word plots I removed the interjection words at the top of the list. Otherwise, you would have seen and,the,for, and have. But the for is a keyword and the data table had to be sliced instead of deselecting those words.


***
***
***

We still haven't done predictive analytics to predict the rating by the review. We will do that next. I would also like to create a visNetwork of these words, with the ratings, and the business type these words are associated with.

The way that sentiment analysis works is to build the document term matrix (dtm) of counts based on the reveiws, and use those counts of words and given ratings to determine the best fit from any particular algorithm that can predict a review as being a specific rating. We have the dtms of all five of our ratings. But we don't have anything set up manually to count all those words from every review or at least any keywords to build those models in predicting our reviews. Normally, you have each row in a dtm is a review, and the columns are each specific word, and evertime that word is found, the word will be added to its last count to get a final count of each word per document. We could do something like this based on our key words. 

We could also quickly jump over to python and wait a bit in running the datatable we cleaned up into a bunch of algorithms like random forest, decision trees, generalized linear models, boosted trees, naive bayes, etc. Or we could look up the text mining and natural language processing packages in the libraries we attached to this document or add to as needed. 

Since this document has been manual from the beginning by cleaning up and extracting features from the reviews. We could just use those features, instead of the words, or we could pick a handful of words, even stopwords, that our program will count in each review, and use as features to predict the reviews with what we already know how to do from previous work in github and rpubs.

Lets look again at the features we do have from our big cleaned up table.
```{r}
colnames(Reviews13)
```

Our target variable would be the 4th column feature above called userRatingValue. We can keep every feature column except the 7th for userReveiwContent that is not our cleaned up review feature and Date. Although, we could get the day of the date feature, because that might have a value added benefit to predicting the rating from these reviews. We also don't need the business Reply Content and won't need the user reviews cleaned up as a predictor once we extract the keyword counts we want. We will just use the words we saw from our word clouds above for a poor, average, or great review subsets. Lets keep the top 10 from each, including the stopwords. 

The poor ratings keywords are for ratings of 1 or 2.
```{r}
KW_poor <- poor %>% select(term,Rating1termfrequency,Rating2termfrequency)
KW_poor$medianLowRate <- apply(KW_poor[2:3],1,median, na.rm=TRUE)
keywords_low <- KW_poor[order(KW_poor$medianLowRate,decreasing=TRUE)[1:10],]
keywords_low
```

We can now use these as our poor keywords.
```{r}
low_keys <- as.data.frame(t(keywords_low$medianLowRate))
colnames(low_keys) <- keywords_low$term
row.names(low_keys) <- 'lowRating'
low_keys
```

And these are our average rating keywords. from the median of 2-4 ratings.
```{r}
KW_avg <- average %>% select(term,Rating2termfrequency,Rating3termfrequency,
                             Rating4termfrequency)
KW_avg$medianAvgRate <- apply(KW_avg[2:4],1,median, na.rm=TRUE)
keywords_avg <- KW_avg[order(KW_avg$medianAvgRate,decreasing=TRUE)[1:10],]
keywords_avg
```

The keywords for the average ratings is a median value of the ratings 2 through 4.
```{r}
avg_keys <- as.data.frame(t(keywords_avg$medianAvgRate))
colnames(avg_keys) <- keywords_avg$term
row.names(avg_keys) <- 'avgRating'
avg_keys
```

Lets get our great ratings as the median of the 4-5 ratings.
```{r}
KW_grt <- goodGreat %>% select(term,Rating5termfrequency,Rating4termfrequency)
KW_grt$medianGrtRate <- apply(KW_grt[2:3],1,median, na.rm=TRUE)
keywords_grt <- KW_grt[order(KW_grt$medianGrtRate,decreasing=TRUE)[1:10],]
keywords_grt
```


The keywords for the great ratings is a median value of the ratings 4 through 5.
```{r}
grt_keys <- as.data.frame(t(keywords_grt$medianGrtRate))
colnames(grt_keys) <- keywords_grt$term
row.names(grt_keys) <- 'grtRating'
grt_keys
```

Now lets combine these tables.
```{r}
j1 <- full_join(grt_keys,avg_keys)
j1
```

```{r}
j2 <- full_join(low_keys,j1)
row.names(j2) <- c('low','great','average')
j2
```

Lets fill in these words manually with their median values. Optionally, we could just take the complete.cases of this table and find those words to use as features.
```{r}
j2$not[2] <- KW_grt[grep('^not$',KW_grt$term),4]
j2$but[2] <- KW_grt[grep('^but$',KW_grt$term),4]

j2$good[1] <- KW_poor[grep('^good$',KW_poor$term),4]
j2$good[3] <- KW_avg[grep('^good$',KW_avg$term),4]

j2$with[1] <- KW_poor[grep('^with$',KW_poor$term),4]
j2$with[3] <- KW_avg[grep('^with$',KW_poor$term),4]
j2

```

But these values are for counts out of the entire count of reviews for each rating. So we should divide each value by the total number of documents to get a ratio or the values in each rating as low, great, or average.
```{r}
keys_t <- as.data.frame(t(j2))
keys_t

```

```{r}
s1 <- sum(Reviews13$userRatingValue==1)+sum(Reviews13$userRatingValue==2)
s2 <- sum(Reviews13$userRatingValue==2)+sum(Reviews13$userRatingValue==3)+
              sum(Reviews13$userRatingValue==4)
s3 <- sum(Reviews13$userRatingValue==4)+sum(Reviews13$userRatingValue==5)

keys_t$low <- round(((keys_t$low)/s1),2)
keys_t$great <- round(((keys_t$great)/s3),2)
keys_t$average <- round(((keys_t$average)/s2),2)
keys_t
```

The above table is for document term frequency on average that is how many times the term shows up in a single document by category of low, average, or great rating. We made these tables earlier, FREQ_R1, ...,FREQ_R5.


What about the ratio for the term against the number in terms in total for all ratings? Lets put that table together.
```{r}
termKeys <- as.data.frame(row.names(keys_t))
colnames(termKeys) <- 'term'

tk1 <- merge(termKeys, FREQ_R1, by.x='term', by.y='term')
tk2 <- merge(tk1,FREQ_R2, by.x='term', by.y='term')
tk3 <- merge(tk2, FREQ_R3, by.x='term', by.y='term')
tk4 <- merge(tk3, FREQ_R4, by.x='term', by.y='term')
tk5 <- merge(tk4, FREQ_R5, by.x='term', by.y='term')

tk5$Rating1_totalTerms <- sum(FREQ_R1$Rating1termfrequency)
tk5$Rating2_totalTerms <- sum(FREQ_R2$Rating2termfrequency)
tk5$Rating3_totalTerms <- sum(FREQ_R3$Rating3termfrequency)
tk5$Rating4_totalTerms <- sum(FREQ_R4$Rating4termfrequency)
tk5$Rating5_totalTerms <- sum(FREQ_R5$Rating5termfrequency)

#these are total terms over all by rating, not unique terms
tk5$Rating1_term2totalTerm <- tk5$Rating1termfrequency/tk5$Rating1_totalTerms
tk5$Rating2_term2totalTerm <- tk5$Rating2termfrequency/tk5$Rating2_totalTerms
tk5$Rating3_term2totalTerm <- tk5$Rating3termfrequency/tk5$Rating3_totalTerms
tk5$Rating4_term2totalTerm <- tk5$Rating4termfrequency/tk5$Rating4_totalTerms
tk5$Rating5_term2totalTerm <- tk5$Rating5termfrequency/tk5$Rating5_totalTerms

termToTotalTerms <- tk5 %>% select(term,Rating1_term2totalTerm,
                                   Rating2_term2totalTerm,
                                   Rating3_term2totalTerm,
                                   Rating4_term2totalTerm,
                                   Rating5_term2totalTerm)
term_to_totalTerms <- round(termToTotalTerms[,2:6],3)
row.names(term_to_totalTerms) <- termToTotalTerms$term
wordToAllWords <- as.data.frame(t(term_to_totalTerms))
wordToAllWords

```

This table is the total word ratio to all words (not unique words) in each subset of ratings 1-5. Lets write this last table out to csv. We will use it later, and this script will be a long one, with manu objects.
```{r}
write.csv(wordToAllWords,'wordToAllWords.csv', row.names=TRUE)
```


Once we get our counts of each word in each review, we can compare it to these words and see if it appears in the document this percent of the time to aid in classifying each review into the correct rating.

Lets use the stringr library's function str_match_all function. Lets clean up the first observation and store it as a string. Then we will use str_match_all to find the exact number of times each keyword is in the review. and put it in our table.
```{r}
str1 <- as.character(paste(Reviews13$userReviewOnlyContent[1]))
str1 <- gsub('[!|.|,|\n|\']',' ',str1,perl=TRUE)
str1 <- gsub('[  ]',' ',str1)
str1 <- trimws(str1, which=c('both'), whitespace='[\t\r\n ]')

totalTerms <- length((strsplit(str1, split=' ')[[1]]))

keys <- row.names(keys_t)

and <- str_match_all(str1,' [aA][nN][dD] ')
AND <- length(and[[1]])

the <- str_match_all(str1,' [tT][hH][eE] ')
THE <- length(the[[1]])

for1 <- str_match_all(str1,' [fF][oO][rR] ')
FOR1 <- length(for1[[1]])

have <- str_match_all(str1,' [hH][aA][vV][eE] ')
HAVE <- length(have[[1]])

that <- str_match_all(str1,' [tT][hH][aA][tT] ')
THAT <- length(that[[1]])

they <- str_match_all(str1,' [tT][hH][eE][yY] ')
THEY <- length(they[[1]])

this <- str_match_all(str1,' [tT][hH][iI][sS] ')
THIS <- length(this[[1]])

you <- str_match_all(str1,' [yY][oO][uU] ')
YOU <- length(you[[1]])

not <- str_match_all(str1,' [nN][oO][tT] ')
NOT <- length(not[[1]])

but <- str_match_all(str1,' [bB][uU][tT] ')
BUT <- length(but[[1]])

good <- str_match_all(str1,' [gG][oO][oO][dD] ')
GOOD <- length(good[[1]])

with <- str_match_all(str1,' [wW][iI][tT][hH] ')
WITH <- length(with[[1]])

values <- as.data.frame(c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH))
row.names(values) <- keys

keyValues <- as.data.frame(t(values))
keyValues2 <- keyValues/totalTerms
keyValues3 <- rbind(keyValues,keyValues2)
row.names(keyValues3) <- c('documentTermCount','term_to_totalDocumentTerms')
keyValues3 <- round(keyValues3,3)
keyValues3
```

Join this table to the wordToAllWords table using dplyr's full join function.
```{r}
joinKeys <- full_join(wordToAllWords,keyValues3)
r1 <- row.names(wordToAllWords)
r2 <- row.names(keyValues3)
names <- c(r1,r2)
row.names(joinKeys) <- names
joinKeys
```

Looking at the table above, we can use the term_to_totalDocumentTerms values of this observation compared to the ratios of the term2totalTerm ratings for each of these 12 words, and choose the rating with the lowest difference or distance between, then to add up the votes for ratings 1-5 for all 12 choices. There should be a clear winner in this algorithm of selecting or predicting the sentiment rating. So, lets try it out.
```{r}
and_diff <- joinKeys$and[1:5]-joinKeys$and[7]
but_diff <- joinKeys$but[1:5]-joinKeys$but[7]
for_diff <- joinKeys[1:5,3]-joinKeys[7,3]
good_diff <- joinKeys$good[1:5]-joinKeys$good[7]
have_diff <- joinKeys$have[1:5]-joinKeys$have[7]
not_diff <- joinKeys$not[1:5]-joinKeys$not[7]
that_diff <- joinKeys$that[1:5]-joinKeys$that[7]
the_diff <- joinKeys$the[1:5]-joinKeys$the[7]
they_diff <- joinKeys$they[1:5]-joinKeys$they[7]
this_diff <- joinKeys$this[1:5]-joinKeys$this[7]
with_diff <- joinKeys$with[1:5]-joinKeys$with[7]
you_diff <- joinKeys$you[1:5]-joinKeys$you[7]

diff <- as.data.frame(t(cbind(and_diff, but_diff, for_diff, good_diff, have_diff, not_diff, 
              that_diff, the_diff, they_diff, this_diff, with_diff, you_diff)))
colnames(diff) <- r1

diff$minValue <- apply(diff,1, min)
diff$vote <- ifelse(diff$Rating1_term2totalTerm==diff$minValue,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue,
                                         4,
                                         5)
                                  )
                           )
                    )

diff$minValue2 <- ifelse(abs(diff$minValue)>abs(diff$Rating1_term2totalTerm),
                         diff$Rating1_term2totalTerm,
                         ifelse(abs(diff$minValue)>abs(diff$Rating2_term2totalTerm),
                                diff$Rating2_term2totalTerm,
                                ifelse(abs(diff$minValue)>abs(diff$Rating3_term2totalTerm),
                                       diff$Rating3_term2totalTerm,
                                       ifelse(abs(diff$minValue)>abs(diff$Rating4_term2totalTerm),
                                              diff$Rating4_term2totalTerm,
                                                ifelse(abs(diff$minValue)>abs(diff$Rating5_term2totalTerm),
                                                  diff$Rating5_term2totalTerm,
                                                   diff$minValue)
                                              )
                                      )
                                )
                          )
  
diff$vote2 <- ifelse(diff$Rating1_term2totalTerm==diff$minValue2,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue2,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue2,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue2,
                                         4,
                                         5)
                                  )
                           )
                    )  


diff
```

There is actually a tie between the review being a 5 or a 1 when using vote 1 that takes the minimum value that includes very negative values. We need to make a rule for when this happens. How about try out for if there is a tie, the best of the median rounded up or the mean rounded down. There is also a vote2 field that takes the shortest distance to the review ratio out of each review and votes for that review. Lets see the results of the first vote with only the minimum.
```{r}
bestVote <- diff %>% group_by(vote) %>% count()
bestVote$maxVote <- ifelse(bestVote$n==max(bestVote$n),
                           1,0)
bestVote$ratingMean <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(mean(bestVote$vote*bestVote$n))>5,
                                 5, ceiling(mean(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )
bestVote$ratingMedian <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(median(bestVote$vote*bestVote$n))>5,
                                 5,ceiling(median(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )

max(bestVote$ratingMean)
max(bestVote$ratingMedian)

```

```{r}
bestVote
```

From the above table, it identified a tie in votes, and calculated the mean and medians of the votes*the count for each vote as a dot product. The mean is actually 7, so a constraint was also placed or wrapped around the ceiling of the mean if it is greater than our highest rating, that it be the highest rating. Same for the median. Lets use Vote2 which takes the shortest distance from the term to Total Term frequency ratio of the review to each ratings term to Total Term frequency ratio. We could choose to accept the mean driven vote of 5 or median driven vote of 4.But lets see how vote2 measures in for predicting most likely reveiw.
```{r}
bestVote2 <- diff %>% group_by(vote2) %>% count()
bestVote2$maxVote2 <- ifelse(bestVote2$n==max(bestVote2$n),
                           1,0)
bestVote2$ratingMean2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(mean(bestVote2$vote2*bestVote2$n))>5,
                                 5, ceiling(mean(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )
bestVote2$ratingMedian2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(median(bestVote2$vote2*bestVote2$n))>5,
                                 5,ceiling(median(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )

max(bestVote2$ratingMean2)
max(bestVote2$ratingMedian2)
```
```{r}
bestVote2
```

When using the shortest distance between the ratio of term to total terms in the review, instead of the minimum distance, the highest votes were not a tie, but for a 1 rating.

Lets see what this rating is. The string object was taken from the first review of the business.
```{r}
Reviews13[1,]

```

It turns out using the highest vote of the shortest distance is not the best in predicting the sentiment, but it looks like the ceiling of the mean of the dot product when a tie based on selecting the minimum value from the difference between ratios in each rating of word to total words in the document subset by rating is best when using the ratio of each review as a ratio of document term to total terms in the document. Lets re-run this script with a different review now and compare results. We only used the words within each rating ratio and most frequent within a broad category of low (1s and 2s), average(2s,3s,and 4s), and great(4s and 5s) only in selecting best keywords by median ratios for each of those three categories for describing each review rating. Also, the stopwords were not excluded like they normally are or in some cases they are.


***
***
***

We are going to test out this algorithm using only the top 12 keywords of three groups but keeping the five ratings to predict by vote. Our best prediction the first run was on breaking a tie with the ceiling of the mean or the highest value of the vote if it is the highest. Lets use the 2nd review this time.
```{r}
str1 <- as.character(paste(Reviews13$userReviewOnlyContent[2]))
str1 <- gsub('[!|.|,|\n|\']',' ',str1,perl=TRUE)
str1 <- gsub('[  ]',' ',str1)
str1 <- trimws(str1, which=c('both'), whitespace='[\t\r\n ]')

totalTerms <- length((strsplit(str1, split=' ')[[1]]))

keys <- row.names(keys_t)

and <- str_match_all(str1,' [aA][nN][dD] ')
AND <- length(and[[1]])

the <- str_match_all(str1,' [tT][hH][eE] ')
THE <- length(the[[1]])

for1 <- str_match_all(str1,' [fF][oO][rR] ')
FOR1 <- length(for1[[1]])

have <- str_match_all(str1,' [hH][aA][vV][eE] ')
HAVE <- length(have[[1]])

that <- str_match_all(str1,' [tT][hH][aA][tT] ')
THAT <- length(that[[1]])

they <- str_match_all(str1,' [tT][hH][eE][yY] ')
THEY <- length(they[[1]])

this <- str_match_all(str1,' [tT][hH][iI][sS] ')
THIS <- length(this[[1]])

you <- str_match_all(str1,' [yY][oO][uU] ')
YOU <- length(you[[1]])

not <- str_match_all(str1,' [nN][oO][tT] ')
NOT <- length(not[[1]])

but <- str_match_all(str1,' [bB][uU][tT] ')
BUT <- length(but[[1]])

good <- str_match_all(str1,' [gG][oO][oO][dD] ')
GOOD <- length(good[[1]])

with <- str_match_all(str1,' [wW][iI][tT][hH] ')
WITH <- length(with[[1]])

values <- as.data.frame(c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH))
row.names(values) <- keys

keyValues <- as.data.frame(t(values))
keyValues2 <- keyValues/totalTerms
keyValues3 <- rbind(keyValues,keyValues2)
row.names(keyValues3) <- c('documentTermCount','term_to_totalDocumentTerms')
keyValues3 <- round(keyValues3,3)
keyValues3
```

Join this table to the wordToAllWords table using dplyr's full join function.
```{r}
joinKeys <- full_join(wordToAllWords,keyValues3)
r1 <- row.names(wordToAllWords)
r2 <- row.names(keyValues3)
names <- c(r1,r2)
row.names(joinKeys) <- names
joinKeys
```

Looking at the table above, we can use the term_to_totalDocumentTerms values of this observation compared to the ratios of the term2totalTerm ratings for each of these 12 words, and choose the rating with the lowest difference or distance between, then to add up the votes for ratings 1-5 for all 12 choices. There should be a clear winner in this algorithm of selecting or predicting the sentiment rating. So, lets try it out.
```{r}
and_diff <- joinKeys$and[1:5]-joinKeys$and[7]
but_diff <- joinKeys$but[1:5]-joinKeys$but[7]
for_diff <- joinKeys[1:5,3]-joinKeys[7,3]
good_diff <- joinKeys$good[1:5]-joinKeys$good[7]
have_diff <- joinKeys$have[1:5]-joinKeys$have[7]
not_diff <- joinKeys$not[1:5]-joinKeys$not[7]
that_diff <- joinKeys$that[1:5]-joinKeys$that[7]
the_diff <- joinKeys$the[1:5]-joinKeys$the[7]
they_diff <- joinKeys$they[1:5]-joinKeys$they[7]
this_diff <- joinKeys$this[1:5]-joinKeys$this[7]
with_diff <- joinKeys$with[1:5]-joinKeys$with[7]
you_diff <- joinKeys$you[1:5]-joinKeys$you[7]

diff <- as.data.frame(t(cbind(and_diff, but_diff, for_diff, good_diff, have_diff, not_diff, 
              that_diff, the_diff, they_diff, this_diff, with_diff, you_diff)))
colnames(diff) <- r1

diff$minValue <- apply(diff,1, min)
diff$vote <- ifelse(diff$Rating1_term2totalTerm==diff$minValue,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue,
                                         4,
                                         5)
                                  )
                           )
                    )

diff$minValue2 <- ifelse(abs(diff$minValue)>abs(diff$Rating1_term2totalTerm),
                         diff$Rating1_term2totalTerm,
                         ifelse(abs(diff$minValue)>abs(diff$Rating2_term2totalTerm),
                                diff$Rating2_term2totalTerm,
                                ifelse(abs(diff$minValue)>abs(diff$Rating3_term2totalTerm),
                                       diff$Rating3_term2totalTerm,
                                       ifelse(abs(diff$minValue)>abs(diff$Rating4_term2totalTerm),
                                              diff$Rating4_term2totalTerm,
                                                ifelse(abs(diff$minValue)>abs(diff$Rating5_term2totalTerm),
                                                  diff$Rating5_term2totalTerm,
                                                   diff$minValue)
                                              )
                                      )
                                )
                          )
  
diff$vote2 <- ifelse(diff$Rating1_term2totalTerm==diff$minValue2,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue2,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue2,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue2,
                                         4,
                                         5)
                                  )
                           )
                    )  


diff
```

We need to make a rule for when this happens. How about try out for if there is a tie, the best of the median rounded up or the mean rounded down. There is also a vote2 field that takes the shortest distance to the review ratio out of each review and votes for that review. Lets see the results of the first vote with only the minimum.
```{r}
bestVote <- diff %>% group_by(vote) %>% count()
bestVote$maxVote <- ifelse(bestVote$n==max(bestVote$n),
                           1,0)
bestVote$ratingMean <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(mean(bestVote$vote*bestVote$n))>5,
                                 5, ceiling(mean(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )
bestVote$ratingMedian <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(median(bestVote$vote*bestVote$n))>5,
                                 5,ceiling(median(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )

max(bestVote$ratingMean)
max(bestVote$ratingMedian)

```
Our best algorithm selected 5 as the best vote, the first run of this program it was a 5 and the mean rating won that prediction.
```{r}
bestVote
```

Lets see how vote2 measures in for predicting most likely reveiw.
```{r}
bestVote2 <- diff %>% group_by(vote2) %>% count()
bestVote2$maxVote2 <- ifelse(bestVote2$n==max(bestVote2$n),
                           1,0)
bestVote2$ratingMean2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(mean(bestVote2$vote2*bestVote2$n))>5,
                                 5, ceiling(mean(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )
bestVote2$ratingMedian2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(median(bestVote2$vote2*bestVote2$n))>5,
                                 5,ceiling(median(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )

max(bestVote2$ratingMean2)
max(bestVote2$ratingMedian2)
```
```{r}
bestVote2
```

Well they have the same results almost for both vote and vote2, except that vote2 this time included all ratings, the first run the rating 3 had no votes, but the vote is the same as a 1 rating, while the ceiling of the mean is a 5 rating, and the ceiling of the median is a 4 rating.

Lets see what this rating is. The string object was taken from the first review of the business.
```{r}
Reviews13[2,]

```

This time, the ceiling of the median using the minimum difference between the document to corpus of each rating ratios of term to total terms in the document versus term to total terms within all documents in each rating.

We should try another, maybe a review closer to the tail to see if the minimum distance is still the best, but choosing mean or median is still a fixer upper. We still haven't used the Reviews13 regular features we spent some time extracting and adding to base what the review's rating will be.
Also, adding a visNetwork link analysis plot to show how the ratings and keywords look or link to each other by weight as the term to total terms ratio, or forgetting these keywords and using the top full join keywords by frequency in each rating. 

***
***
***

Lets re-run this script on another review closer to the tail to see how the results are predicted.
```{r}
str1 <- as.character(paste(Reviews13$userReviewOnlyContent[600]))
str1 <- gsub('[!|.|,|\n|\']',' ',str1,perl=TRUE)
str1 <- gsub('[  ]',' ',str1)
str1 <- trimws(str1, which=c('both'), whitespace='[\t\r\n ]')

totalTerms <- length((strsplit(str1, split=' ')[[1]]))

keys <- row.names(keys_t)

and <- str_match_all(str1,' [aA][nN][dD] ')
AND <- length(and[[1]])

the <- str_match_all(str1,' [tT][hH][eE] ')
THE <- length(the[[1]])

for1 <- str_match_all(str1,' [fF][oO][rR] ')
FOR1 <- length(for1[[1]])

have <- str_match_all(str1,' [hH][aA][vV][eE] ')
HAVE <- length(have[[1]])

that <- str_match_all(str1,' [tT][hH][aA][tT] ')
THAT <- length(that[[1]])

they <- str_match_all(str1,' [tT][hH][eE][yY] ')
THEY <- length(they[[1]])

this <- str_match_all(str1,' [tT][hH][iI][sS] ')
THIS <- length(this[[1]])

you <- str_match_all(str1,' [yY][oO][uU] ')
YOU <- length(you[[1]])

not <- str_match_all(str1,' [nN][oO][tT] ')
NOT <- length(not[[1]])

but <- str_match_all(str1,' [bB][uU][tT] ')
BUT <- length(but[[1]])

good <- str_match_all(str1,' [gG][oO][oO][dD] ')
GOOD <- length(good[[1]])

with <- str_match_all(str1,' [wW][iI][tT][hH] ')
WITH <- length(with[[1]])

values <- as.data.frame(c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH))
row.names(values) <- keys

keyValues <- as.data.frame(t(values))
keyValues2 <- keyValues/totalTerms
keyValues3 <- rbind(keyValues,keyValues2)
row.names(keyValues3) <- c('documentTermCount','term_to_totalDocumentTerms')
keyValues3 <- round(keyValues3,3)
keyValues3
```

Join this table to the wordToAllWords table using dplyr's full join function.
```{r}
joinKeys <- full_join(wordToAllWords,keyValues3)
r1 <- row.names(wordToAllWords)
r2 <- row.names(keyValues3)
names <- c(r1,r2)
row.names(joinKeys) <- names
joinKeys
```

Looking at the table above, we can use the term_to_totalDocumentTerms values of this observation compared to the ratios of the term2totalTerm ratings for each of these 12 words, and choose the rating with the lowest difference or distance between, then to add up the votes for ratings 1-5 for all 12 choices. There should be a clear winner in this algorithm of selecting or predicting the sentiment rating. So, lets try it out.
```{r}
and_diff <- joinKeys$and[1:5]-joinKeys$and[7]
but_diff <- joinKeys$but[1:5]-joinKeys$but[7]
for_diff <- joinKeys[1:5,3]-joinKeys[7,3]
good_diff <- joinKeys$good[1:5]-joinKeys$good[7]
have_diff <- joinKeys$have[1:5]-joinKeys$have[7]
not_diff <- joinKeys$not[1:5]-joinKeys$not[7]
that_diff <- joinKeys$that[1:5]-joinKeys$that[7]
the_diff <- joinKeys$the[1:5]-joinKeys$the[7]
they_diff <- joinKeys$they[1:5]-joinKeys$they[7]
this_diff <- joinKeys$this[1:5]-joinKeys$this[7]
with_diff <- joinKeys$with[1:5]-joinKeys$with[7]
you_diff <- joinKeys$you[1:5]-joinKeys$you[7]

diff <- as.data.frame(t(cbind(and_diff, but_diff, for_diff, good_diff, have_diff, not_diff, 
              that_diff, the_diff, they_diff, this_diff, with_diff, you_diff)))
colnames(diff) <- r1

diff$minValue <- apply(diff,1, min)
diff$vote <- ifelse(diff$Rating1_term2totalTerm==diff$minValue,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue,
                                         4,
                                         5)
                                  )
                           )
                    )

diff$minValue2 <- ifelse(abs(diff$minValue)>abs(diff$Rating1_term2totalTerm),
                         diff$Rating1_term2totalTerm,
                         ifelse(abs(diff$minValue)>abs(diff$Rating2_term2totalTerm),
                                diff$Rating2_term2totalTerm,
                                ifelse(abs(diff$minValue)>abs(diff$Rating3_term2totalTerm),
                                       diff$Rating3_term2totalTerm,
                                       ifelse(abs(diff$minValue)>abs(diff$Rating4_term2totalTerm),
                                              diff$Rating4_term2totalTerm,
                                                ifelse(abs(diff$minValue)>abs(diff$Rating5_term2totalTerm),
                                                  diff$Rating5_term2totalTerm,
                                                   diff$minValue)
                                              )
                                      )
                                )
                          )
  
diff$vote2 <- ifelse(diff$Rating1_term2totalTerm==diff$minValue2,
                    1, 
                    ifelse(diff$Rating2_term2totalTerm==diff$minValue2,
                           2,
                           ifelse(diff$Rating3_term2totalTerm==diff$minValue2,
                                  3,
                                  ifelse(diff$Rating4_term2totalTerm==diff$minValue2,
                                         4,
                                         5)
                                  )
                           )
                    )  


diff
```

Lets see the results of the first vote with only the minimum.
```{r}
bestVote <- diff %>% group_by(vote) %>% count()
bestVote$maxVote <- ifelse(bestVote$n==max(bestVote$n),
                           1,0)
bestVote$ratingMean <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(mean(bestVote$vote*bestVote$n))>5,
                                 5, ceiling(mean(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )
bestVote$ratingMedian <- ifelse(sum(bestVote$maxVote) > 1,
                          ifelse(ceiling(median(bestVote$vote*bestVote$n))>5,
                                 5,ceiling(median(bestVote$vote*bestVote$n))), 
                           ifelse(bestVote$n==max(bestVote$n),
                                  bestVote$vote,
                                  0)
                          )

max(bestVote$ratingMean)
max(bestVote$ratingMedian)

```

```{r}
bestVote
```

Lets see how vote2 measures in for predicting most likely reveiw.
```{r}
bestVote2 <- diff %>% group_by(vote2) %>% count()
bestVote2$maxVote2 <- ifelse(bestVote2$n==max(bestVote2$n),
                           1,0)
bestVote2$ratingMean2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(mean(bestVote2$vote2*bestVote2$n))>5,
                                 5, ceiling(mean(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )
bestVote2$ratingMedian2 <- ifelse(sum(bestVote2$maxVote2) > 1,
                          ifelse(ceiling(median(bestVote2$vote2*bestVote2$n))>5,
                                 5,ceiling(median(bestVote2$vote2*bestVote2$n))), 
                           ifelse(bestVote2$n==max(bestVote2$n),
                                  bestVote2$vote2,
                                  0)
                          )

max(bestVote2$ratingMean2)
max(bestVote2$ratingMedian2)
```
```{r}
bestVote2
```

These keywords aren't really showing much benefit, since they keep having the same result. A 1 rating for vote2 by shortest distance between review to all reviews' term to total terms ratio and a 5 for ceiling of the mean of dot product of votes and ratings by minimum values of ratio differences, and a 4 for the ceiling of the median of dot product of votes and ratings by minimum values of ratio differences.

Lets see what this rating is. The string object was taken from the first review of the business.
```{r}
Reviews13[600,]

```

The rating is actually a 5. So, the ceiling of the mean value of the dot product of votes by ratings selected by minimum value of ratio of review to ratio of reviews in each rating using each term to total terms in the single review to each collection of reviews in each rating of 1-5.

We can later add these features to each observation, make 70% a trainging set to build and train the model to predict the rating. Then use the other 30% the testing set to test the model built on the other partitioned 70% to predict the rating. We would have to run the best model above which used the mean of dot product of the votes by ratings, then the ceiling of that value which rounds up on all measures. As this algorithm was more accurate in predicting the rating.We could either do that now, or step away for a while to let it simmer and see what this data on reviews and key words looks like in a link analysis plot. Why don't we do the latter and do it.

***
***
***

Lets visualize these keywords by the layout of these keywords to rating by ratios as weight, ratings as edges, and nodes as keywords. We have to first take this information from the data table on the ratios of terms counted each per documents in each rating to total terms per documents per rating. This data table is the wordToAllWords table. From this table we have our weights and our ratings. The weights are what will make the arrows width smaller or larger than the other arrow widths depending on how much weight they have on each word linked to a specific rating. The ratings are the edges. The nodes are the words, and those are also in this table. The label in the nodes table will be the keyword, and the title will be the rating. The id is the row number from the nodes table, which is the from column in the edges table. and the to column in the edges table will be the rating. Lets also add another feature from the Reviews13 data table for the day of the week as Monday through Sunday by adding a feature that takes the day of the week from the date field we added earlier in the data. Lets read in those two data tables and make sure our libraries are loaded in to Rstudio from the top of this script.The visNetwork and igraph link analysis and visualization libraries will be used for this link analysis. The package igraph makes the visNetwork package work faster in uploading and allows editing and modifying the link analysis network using various customized plot layouts and color schemes as well as other added value to the visualization.

Lets add the day of the week using the lubridate package to the Reviews13 datatable Date field.
```{r}
head(Reviews13)
```

When looking at the table above there are other factors that could be grouped by, such as the business type, the cost as low, average, or high, then number of photos each user has, etc. But we will just focus on using the day of the week. Lets add the day of the week now. First lets make sure the Date feature is recognized as a date feature.
```{r}
class(Reviews13$Date)
```

It is a factor, so we will change it to a date feature type.
```{r}
Reviews13$Date <- as.Date(Reviews13$Date)
class(Reviews13$Date)
```

Now lets extract the day of the week from our date feature.
```{r}
date <- ymd(Reviews13$Date)
date <- day(Reviews13$Date)

Reviews13$weekday <- wday(date, label=TRUE, week_start=1)#set start of week to Monday)
head(Reviews13$weekday)
```

Now, we could have attached this information to the count of keywords in each review, but we skipped that step, and we have to make the process automated with a for loop to take every row in data table feature and keep applying this string filter program that counts the 12 words in each observation, and returns a vector that is row binded to the previous vector until now more observations left. We could do that later, you could do it now, or we could try it now and see if it is as simple as it sounds to set up. I will try it once, and if it will take more manipulation, or time, we will just work with what is readily available to focus on the link analysis network design we already planned and created instructions though loose on how to create.
This is the keyword extraction script:
```{r}
str1 <- as.character(paste(Reviews13$userReviewOnlyContent[600]))
str1 <- gsub('[!|.|,|\n|\']',' ',str1,perl=TRUE)
str1 <- gsub('[  ]',' ',str1)
str1 <- trimws(str1, which=c('both'), whitespace='[\t\r\n ]')

totalTerms <- length((strsplit(str1, split=' ')[[1]]))

keys <- row.names(keys_t)

and <- str_match_all(str1,' [aA][nN][dD] ')
AND <- length(and[[1]])

the <- str_match_all(str1,' [tT][hH][eE] ')
THE <- length(the[[1]])

for1 <- str_match_all(str1,' [fF][oO][rR] ')
FOR1 <- length(for1[[1]])

have <- str_match_all(str1,' [hH][aA][vV][eE] ')
HAVE <- length(have[[1]])

that <- str_match_all(str1,' [tT][hH][aA][tT] ')
THAT <- length(that[[1]])

they <- str_match_all(str1,' [tT][hH][eE][yY] ')
THEY <- length(they[[1]])

this <- str_match_all(str1,' [tT][hH][iI][sS] ')
THIS <- length(this[[1]])

you <- str_match_all(str1,' [yY][oO][uU] ')
YOU <- length(you[[1]])

not <- str_match_all(str1,' [nN][oO][tT] ')
NOT <- length(not[[1]])

but <- str_match_all(str1,' [bB][uU][tT] ')
BUT <- length(but[[1]])

good <- str_match_all(str1,' [gG][oO][oO][dD] ')
GOOD <- length(good[[1]])

with <- str_match_all(str1,' [wW][iI][tT][hH] ')
WITH <- length(with[[1]])

values <- as.data.frame(c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH))
row.names(values) <- keys

keyValues <- as.data.frame(t(values))
keyValues2 <- keyValues/totalTerms
keyValues3 <- rbind(keyValues,keyValues2)
row.names(keyValues3) <- c('documentTermCount','term_to_totalDocumentTerms')
keyValues3 <- round(keyValues3,3)
keyValues3
```

We see from this output per review, that we could first add a paste function to attach a new name to every review by row number in the data table Reviews13. We need to make sure they are the correct row number values by order of their listed review.
```{r}
row.names(Reviews13) <- NULL
row.names(Reviews13) <- as.character(paste(row.names(Reviews13)))
head(row.names(Reviews13))
```

%^&%^&
Looks like they are ordered. There are 614 reviews to extract the 12 keyword counts and ratio of words per document to total words per document. Lets try wrapping this up in a for loop.
```{r,eval=FALSE}
str <- as.character(paste(Reviews13$userReviewOnlyContent))
for (review in (str))
  { 
    
      s <- gsub('[!|.|,|\n|\']',' ',review,perl=TRUE)
      s <- gsub('[  ]',' ',s)
      s <- trimws(s, which=c('both'), whitespace='[\t\r\n ]')
     # s <- strsplit(s, split=' ') #this is where the error is, outside the loop is fine

      totalTerms <- length(strsplit(s, split=' ')[[1]])
      

      and <- str_match_all(s,' [aA][nN][dD] ')
      AND <- length(and[[1]])

      the <- str_match_all(s,' [tT][hH][eE] ')
      THE <- length(the[[1]])

      for1 <- str_match_all(s,' [fF][oO][rR] ')
      FOR1 <- length(for1[[1]])

      have <- str_match_all(s,' [hH][aA][vV][eE] ')
      HAVE <- length(have[[1]])

      that <- str_match_all(s,' [tT][hH][aA][tT] ')
      THAT <- length(that[[1]])

      they <- str_match_all(s,' [tT][hH][eE][yY] ')
      THEY <- length(they[[1]])

      this <- str_match_all(s,' [tT][hH][iI][sS] ')
      THIS <- length(this[[1]])

      you <- str_match_all(s,' [yY][oO][uU] ')
      YOU <- length(you[[1]])

      not <- str_match_all(s,' [nN][oO][tT] ')
      NOT <- length(not[[1]])

      but <- str_match_all(s,' [bB][uU][tT] ')
      BUT <- length(but[[1]])

      good <- str_match_all(s,' [gG][oO][oO][dD] ')
      GOOD <- length(good[[1]])

      with <- str_match_all(s,' [wW][iI][tT][hH] ')
      WITH <- length(with[[1]])

      values1 <- c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH)

      values2 <- values1/totalTerms
      
      ##cat function to save the values and work out another way to read in and combine
      cat(values1,file="values1.csv",sep="\n",append ="TRUE",fill=TRUE)
      cat(values2,file="values2.csv",sep="\n",append ="TRUE",fill=TRUE)
}

#loop fails at the splitstr function that was commented out. 
```

I played around with the for loop longer than expected, and it needs more work. It isn't doing what I want it to do. I will manually get a handful of values later as needed, or move on to the vis network. I fixed the code, but this chunk won't evaluate. The script will be assumed to err until we encounter it again so as to move on to the visNetwork link analysis plots. 


***
***
***

Lets move onto the visNetwork. We are using the wordToAllWords and Reviews13 tables. Lets select our columns from each.These were both written to csv earlier as ReviewsCleanedWithKeywordsAndRatios.csv for Reviews13 and wordToAllWords.csv for that table. If you cleaned out your environment and left or shut down Rstudio then you can read in these two as their table names and test the script below.
```{r}
visNodes <- Reviews13 %>% select(userRatingValue,LowAvgHighCost, businessType,weekday)

visNodes$label <- visNodes$userRatingValue
visNodes$label <- paste('rate',visNodes$label,sep='')

visNodes$title <- visNodes$LowAvgHighCost
visNodes$title <- paste(visNodes$title,'Cost',sep='')

visNodes$group <- visNodes$weekday

visEdges <- as.data.frame(t(wordToAllWords ))
colnames(visEdges) <- c('rate1','rate2','rate3','rate4','rate5')
visEdges$label <- row.names(visEdges)

#the weight is the ratio term2alltermsPerRating
visEdges <- gather(visEdges, 'rating','weight', 1:5) 
head(visNodes)
```

```{r}
Nodes1 <- visNodes %>% select(label,title,group)
head(Nodes1)
```

It moves from 614 to 7368 obsrevations because of the 12 keywords and 614 reveiws which equals 7368.
```{r}
Nodes2 <- merge(Nodes1, visEdges, by.x='label', by.y='rating')
Nodes2$id <- as.factor(paste(row.names(Nodes2)))
Nodes2$term <- Nodes2$label.y
Nodes3 <- Nodes2 %>% select(id,label,title,group,term)
Nodes3$label <- as.factor(paste(Nodes3$label))
Nodes3$term <-  as.factor(paste(Nodes3$term))         
Nodes3$title <- as.factor(paste(Nodes3$title))
head(Nodes3)
```

visEdges only has 60 because it was 5 ratings ratios foe 12 words each.
```{r}
visEdges$label <- as.factor(paste(visEdges$label))
visEdges$rating <- as.factor(paste(visEdges$rating))
head(visEdges)
```

Because there are only 60 fields in the edges and 7368 in the nodes, there will be errors for those values in the nodes that don't have values in the edges. But we will still have a plot. I will suppress these warnings messages within the chunk.The nodes have to have unique IDs but the edges don't.
```{r, error=FALSE, warning=FALSE, message=FALSE}
Edges2 <- visEdges %>% mutate(from=plyr::mapvalues(visEdges$rating,
                                                 from=Nodes3$label,to=Nodes3$id))
Edges3 <- Edges2 %>% mutate(to=plyr::mapvalues(Edges2$label, 
                                               from=Nodes3$term, to=Nodes3$id))

Edges4 <- Edges3 %>% select(from,to,label,weight)
head(Edges4,20)
```

Now lets use visNetwork and igraph to plot these nodes and edges.
```{r}

visNetwork(nodes=Nodes3, edges=Edges4, main='Weekday Groups of Rating and 12 Keywords') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=FALSE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```

The above is very large because it has the added groups and keywords of 12 to run combinations against the original 614 observations. Lets try limiting the observations.

***

We will work from the beginning of the last visNetwork plot.
```{r}
visNodes <- Reviews13 %>% select(userRatingValue,LowAvgHighCost, businessType,weekday)

visNodes$label <- visNodes$userRatingValue
visNodes$label <- paste('rate',visNodes$label,sep='')

visNodes$title <- visNodes$LowAvgHighCost
visNodes$title <- paste(visNodes$title,'Cost',sep='')

visNodes$group <- visNodes$weekday
head(visNodes)
```

```{r}
visEdges <- as.data.frame(t(wordToAllWords ))
colnames(visEdges) <- c('rate1','rate2','rate3','rate4','rate5')
visEdges$label <- row.names(visEdges)

#the weight is the ratio term2alltermsPerRating
visEdges <- gather(visEdges, 'rating','weight', 1:5) 
head(visEdges)
```

```{r}
Nodes1 <- visNodes %>% select(weekday:group)
head(Nodes1)
```

```{r}
Nodes2 <- merge(Nodes1, visEdges, by.x='label', by.y='rating')
Nodes2$term <- Nodes2$label.y
Nodes2$id <- row.names(Nodes2)
Nodes3 <- Nodes2 %>% select(id,label,title,group,term,weight)
head(Nodes3)
```

Now subset from Nodes3 and make the edges table from this table.
```{r}
Nodes4 <- subset(Nodes3, (Nodes3$group=='Mon'|Nodes3$group=='Wed'|Nodes3$group=='Sat') &
                   (Nodes3$term=='this'|Nodes3$term=='they'|Nodes3$term=='have'|
                      Nodes3$term=='you'|Nodes3$term=='not'|Nodes3$term=='good'))
row.names(Nodes4) <- NULL
Nodes4$id <- as.factor(row.names(Nodes4))
head(Nodes4)
```

```{r}
Edges1 <- Nodes4 %>% select(label,term,group,weight)
Edges2 <- Edges1 %>% mutate(from=plyr::mapvalues(Edges1$label, 
                                                 from=Nodes4$label,to=Nodes4$id))
Edges3 <- Edges2 %>% mutate(to=plyr::mapvalues(Edges2$term, 
                                               from=Nodes4$term, to=Nodes4$id))
Edges4 <- Edges3 %>% select(from,to,label,term,group,weight)
Edges4$label <- Edges4$term
head(Edges4)
```


```{r}

visNetwork(nodes=Nodes4, edges=Edges4, main='Three Weekday Groups of Five Ratings and Five Keywords') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=FALSE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```


Lets make another visualization on price and ratings with terms omitted. I used five keywords instead of three as planned.

We well use the Reviews13 data table. And select the features needed.
```{r}
visNodes3 <- Reviews13 %>% select(userRatingValue,LowAvgHighCost,businessReplied,friends)

visNodes3$id <- row.names(visNodes3)
visNodes3$weight <- visNodes3$friends/max(visNodes3$friends,na.rm=TRUE)
visNodes3$group <- visNodes3$LowAvgHighCost
visNodes3$label <- as.factor(paste('rating', visNodes3$userRatingValue, sep=' '))
visNodes3$title <- visNodes3$businessReplied

head(visNodes3)
```

```{r}
nodes1 <- visNodes3 %>% select(id,label,title, group)

edges1 <- visNodes3 %>% select(id,label,group,weight)
edges1$from <- edges1$id
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges1$group, from=nodes1$group, to = nodes1$id))

edges3 <- edges2 %>% select(from,to,label, group,weight)
head(edges3)
```

```{r}
head(nodes1)
```

```{r}

visNetwork(nodes=nodes1, edges=edges3, main='Ratings Cost if business replied and Number of Friends as arrow weights') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=FALSE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend

```

We can see in the above plot of the ratings in groups by cost of either high, average, or low, that there are almost the same amount of reviews in each group. When hovering the nodes are going to show if the business replied to his or her review as yes if they did and as no if not. When zooming in on the nodes of each group you can see the rating and arrow weights of the number of friends each review has as ratios of the number of friends a user has divided by the max number of friends all users have.


Lets build another network but with different groupings.
```{r}
visNodes3 <- Reviews13 %>% select(userRatingValue,LowAvgHighCost,businessReplied,friends)

visNodes3$id <- row.names(visNodes3)
visNodes3$weight <- visNodes3$friends/max(visNodes3$friends,na.rm=TRUE)
visNodes3$label <- as.factor(paste(visNodes3$LowAvgHighCost,'cost',sep=' '))
visNodes3$group <- as.factor(paste('rating', visNodes3$userRatingValue, sep=' '))
visNodes3$title <- paste(visNodes3$friends,'friends',sep=' ')

head(visNodes3)
```


```{r}
nodes1 <- visNodes3 %>% select(id,label,title, group)

edges1 <- visNodes3 %>% select(id,label,group,weight)
edges1$from <- edges1$id
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges1$group, from=nodes1$group, to = nodes1$id))

edges3 <- edges2 %>% select(from,to,label, group,weight)
head(edges3)
```

```{r}
head(nodes1)

```

```{r}

visNetwork(nodes=nodes1, edges=edges3, main='Ratings as Groups and Cost as Labels with Number of Friends as Arrow Weights') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=FALSE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend(ncol=2)

```




The above visual network is great for looking at the number of reviews in each rating, but also when zooming in to see the cost as Low, High, or Average and hovering shows how many social media friends each reviewer has.

***
***
***


We should make a visual network of the keywords and the ratings to go with and maybe a couple different visualizations on our manually built best model for predicting ratings based on the ceiling of the median of the dot product of votes times ratings when there is a tie between rating votes that were voted on by which review has the minimum value of the ratio of term to total terms in the document to term to total terms by rating.

I worked on the for loop outside this script and thankfully it works and it doesn't take very long for it to run. Less than two minutes. Here is that content and the new file Reviews15 we will be working with. 

!!! CAUTION: !!!

Make sure to **only run this once** if you already have these files or delete the keyValues3.csv and the keyValues3_ratios, as it will append to your files. When running the chunks even with eval set to FALSE, everything is ran, those header commands only work when kitting the file. It takes about a minute to load all 614 reviews into those 12 keywords with ratios. So it is ok to delete the files.

```{r}
for (num in 1:length(Reviews13$userReviewOnlyContent)){
#num <- 30
str1 <- as.character(paste(Reviews13$userReviewOnlyContent[num]))
str1 <- gsub('[!|.|,|\n|\']',' ',str1,perl=TRUE)
str1 <- gsub('[  ]',' ',str1)
str1 <- trimws(str1, which=c('both'), whitespace='[\t\r\n ]')

totalTerms <- length((strsplit(str1, split=' ')[[1]]))

keys <- c("the",  "and" , "for" , "have" ,"that" ,"they" ,"this" ,"you" , 
          "not" , "but"  ,"good" ,"with")

and <- str_match_all(str1,' [aA][nN][dD] ')
AND <- length(and[[1]])

the <- str_match_all(str1,' [tT][hH][eE] ')
THE <- length(the[[1]])

for1 <- str_match_all(str1,' [fF][oO][rR] ')
FOR1 <- length(for1[[1]])

have <- str_match_all(str1,' [hH][aA][vV][eE] ')
HAVE <- length(have[[1]])

that <- str_match_all(str1,' [tT][hH][aA][tT] ')
THAT <- length(that[[1]])

they <- str_match_all(str1,' [tT][hH][eE][yY] ')
THEY <- length(they[[1]])

this <- str_match_all(str1,' [tT][hH][iI][sS] ')
THIS <- length(this[[1]])

you <- str_match_all(str1,' [yY][oO][uU] ')
YOU <- length(you[[1]])

not <- str_match_all(str1,' [nN][oO][tT] ')
NOT <- length(not[[1]])

but <- str_match_all(str1,' [bB][uU][tT] ')
BUT <- length(but[[1]])

good <- str_match_all(str1,' [gG][oO][oO][dD] ')
GOOD <- length(good[[1]])

with <- str_match_all(str1,' [wW][iI][tT][hH] ')
WITH <- length(with[[1]])

values <- as.data.frame(c(THE,AND,FOR1,HAVE,THAT,THEY,THIS,YOU,NOT,BUT,GOOD,WITH))
row.names(values) <- keys

keyValues <- as.data.frame(t(values))
keyValues2 <- keyValues/totalTerms
keyValues3 <- rbind(keyValues,keyValues2)
row.names(keyValues3) <- c(paste('documentTermCount', num, sep='_'),
                           paste('term_to_totalDocumentTerms', num, sep='_'))
keyValues3 <- round(keyValues3,3)

keyValues4 <- as.matrix(keyValues3)
cat(keyValues4[1,1:12],file='keyValues3.csv',append=TRUE, sep='\n',fill=TRUE)
cat(keyValues4[2,1:12],file='keyValues3_ratios.csv',append=TRUE, sep='\n',fill=TRUE)
}

```

Well, great news! The above looks like it worked and now we have the rest of our keyword data to make a matrix and then data frame out of. It took about one minute as I watched the kb file size in the file window change for each keyValues csv file in the for loop above. These are actually all the records, because they were appended to the other records. So we should have 614X12= `r614*12` observations. Lets find out.
```{r}
all_kws <- read.csv('keyValues3.csv', sep=',', header=FALSE, na.strings=c('',' ','NA'))
```

Ok, good, because it does say 7368 obs and 1 variable, 
(if you have more rows than this, you ran the code twice. search for keywords3.csv within Rstudio with the magnifying glass in the toolbar and see if you did, otherwise continue)
as expected or anticipating it to.
Now lets make this into a data frame after first making it into a matrix.
```{r}
all_kws1 <- all_kws$V1
ALL_kws <- matrix(all_kws1, nrow=12,ncol=614,byrow=FALSE)
ALL_KWs <- as.data.frame(t(ALL_kws))
row.names(ALL_KWs) <- NULL
colnames(ALL_KWs) <- keys
```

Now lets get the ratios for all of these reviews and keywords.
```{r}
all_kwrs <- read.csv('keyValues3_ratios.csv', header=FALSE, sep=',',
                     na.strings=c('',' ','NA'))

```

```{r}
all_kwrs1 <- all_kwrs$V1
ALL_kwrs <- matrix(all_kwrs1, nrow=12,ncol=614,byrow=FALSE)
ALL_KWRs <- as.data.frame(t(ALL_kwrs))
row.names(ALL_KWRs) <- NULL
colnames(ALL_KWRs) <- paste(keys,'ratios', sep='_')
```

Now lets combine the two tables together.
```{r}
ALL_keywords <- cbind(ALL_KWs,ALL_KWRs)
head(ALL_keywords)
```

Super great! These for loops can be tricky. Thankfully, that worked. Lets write this out to file. The row names are not important because they are the order listed the same as the reviews listed from the Reviews13 data table.
```{r}
write.csv(ALL_keywords,'ALL_keywords.csv', row.names=FALSE)
```

Lets now combine this with a merge of IDs as row numbers shall we?
```{r}
Reviews14 <- Reviews13
Reviews14$id <- row.names(Reviews13)
ALL_keywords$id <- row.names(ALL_keywords)
```

Now merge by id.
```{r}
Reviews15 <- merge(Reviews14, ALL_keywords, by.x='id', by.y='id')
head(Reviews15)
```

Now we need to write this out to csv to read in to other file or just to have.
```{r}
write.csv(Reviews15,'ReviewsCleanedWithKeywordsAndRatios.csv', row.names=FALSE)
```

This file is almost 1 Mb in file size, which actually isn't that large.

***

Now that we have our table with the added ratios we can now look at building different data sets with feature selection to predict any of our other targets, but most likely the rating by review. But also we can now use our best prediction model of the mean votes algorithm and other big name machine learning algorithms I have used many times. We will do that next. [Caret Cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/caret.pdf) is available from the available cheatsheets in the toolbar in Rstudio in the Help menu under Cheatsheets, scroll to the 'Contributed Cheatsheets' at the bottom and select the caret cheatsheet if you would like to refresh or learn about the algorithms caret has to work with in R for machine learning on the above data set. 

Before moving on to the machine learning algorithms in R, specifically the caret package. Lets make a test and train set out of this data frame, and test the features we individually select on the target variable of the rating using our best model, the ceiling of the mean of the dot product of the votes and ratings or the top votes.We will use our database Reviews15.
If you don't have it in your environment go ahead and read it in from the ReviewsCleanedWithKeywordsAndRatios.csv file.
```{r}

Reviews15 <- read.csv('ReviewsCleanedWithKeywordsAndRatios.csv',sep=',',
                      header=TRUE, na.strings=c('',' ','NA'))
wordToAllWords <- read.csv('wordToAllWords.csv', sep=',', header=TRUE, na.strings=c('',' ','NA'),
                           row.names=1)
```

Now lets see the table we will compare each review ratio of term to total terms to the term to total terms in all reviews by rating.
```{r}
head(wordToAllWords)
```

Now lets select our features we want to use the top model on. Since our model was based on the ratios, we will only be using the rating and those features, and from their it will create a voting system to grab the rating with the highest votes for the term to total terms per document. Make sure to read in all the packages this Rmarkdown file uses. The following will use stringr, dplyr, and tidyr or tidyverse for sure. Later we will use the caret package for Machine learning models to compare against our model.
```{r}
colnames(Reviews15)
```

```{r}
ML_rating_data <- Reviews15 %>% select(userRatingValue,
                                       the_ratios:with_ratios)
head(ML_rating_data)
```


If, or when, we use the caret algorithms to make predictions with many of the available models in caret, the target variable would be the userRatingValue, and the keyword ratios (12) would be the predictors. But we aren't right at this moment and so we don't have to separate the target from the predictors just yet. We can keep it attached to compare to our model we build on the ceiling of the mean if a tie in votes for the minimum values of the review ratio to the five rating rations.

Keep the rating column as an integer instead of a factor, because we need it as an integer to use the dot product on those reviews that there is a tie against the votes (the minimum value of the difference between the term to total terms in the document to the ratios of the same for all the documents in each rating). We saw earlier that there are some instances where there is a tie, but that the ceiling of the mean or the highest rating will beat out the ceiling of the median or the shortest distance (absolute value) between the reveiw ratio and rating ratios. We need to also get our data frame on ratios of total of each of these 12 words or terms to the total of all words in each corpus or file of reviews by rating in the wordToAllWords table. 
```{r}
colnames(wordToAllWords)[3] <- 'for_' #this is special keyword in R
w2w <- wordToAllWords #shorten name
MLr <- ML_rating_data #shorten the name
```

```{r}
MLr$R1_and <- rep(w2w[1,1],length(MLr$userRatingValue))
MLr$R2_and <- rep(w2w[2,1],length(MLr$userRatingValue))
MLr$R3_and <- rep(w2w[3,1],length(MLr$userRatingValue))
MLr$R4_and <- rep(w2w[4,1],length(MLr$userRatingValue))
MLr$R5_and <- rep(w2w[5,1],length(MLr$userRatingValue))

MLr$R1_but <- rep(w2w[1,2],length(MLr$userRatingValue))
MLr$R2_but <- rep(w2w[2,2],length(MLr$userRatingValue))
MLr$R3_but <- rep(w2w[3,2],length(MLr$userRatingValue))
MLr$R4_but <- rep(w2w[4,2],length(MLr$userRatingValue))
MLr$R5_but <- rep(w2w[5,2],length(MLr$userRatingValue))

MLr$R1_for <- rep(w2w[1,3],length(MLr$userRatingValue))
MLr$R2_for <- rep(w2w[2,3],length(MLr$userRatingValue))
MLr$R3_for <- rep(w2w[3,3],length(MLr$userRatingValue))
MLr$R4_for <- rep(w2w[4,3],length(MLr$userRatingValue))
MLr$R5_for <- rep(w2w[5,3],length(MLr$userRatingValue))

MLr$R1_good <- rep(w2w[1,4],length(MLr$userRatingValue))
MLr$R2_good <- rep(w2w[2,4],length(MLr$userRatingValue))
MLr$R3_good <- rep(w2w[3,4],length(MLr$userRatingValue))
MLr$R4_good <- rep(w2w[4,4],length(MLr$userRatingValue))
MLr$R5_good <- rep(w2w[5,4],length(MLr$userRatingValue))

MLr$R1_have <- rep(w2w[1,5],length(MLr$userRatingValue))
MLr$R2_have <- rep(w2w[2,5],length(MLr$userRatingValue))
MLr$R3_have <- rep(w2w[3,5],length(MLr$userRatingValue))
MLr$R4_have <- rep(w2w[4,5],length(MLr$userRatingValue))
MLr$R5_have <- rep(w2w[5,5],length(MLr$userRatingValue))

MLr$R1_not <- rep(w2w[1,6],length(MLr$userRatingValue))
MLr$R2_not <- rep(w2w[2,6],length(MLr$userRatingValue))
MLr$R3_not <- rep(w2w[3,6],length(MLr$userRatingValue))
MLr$R4_not <- rep(w2w[4,6],length(MLr$userRatingValue))
MLr$R5_not <- rep(w2w[5,6],length(MLr$userRatingValue))

MLr$R1_that <- rep(w2w[1,7],length(MLr$userRatingValue))
MLr$R2_that <- rep(w2w[2,7],length(MLr$userRatingValue))
MLr$R3_that <- rep(w2w[3,7],length(MLr$userRatingValue))
MLr$R4_that <- rep(w2w[4,7],length(MLr$userRatingValue))
MLr$R5_that <- rep(w2w[5,7],length(MLr$userRatingValue))

MLr$R1_the <- rep(w2w[1,8],length(MLr$userRatingValue))
MLr$R2_the <- rep(w2w[2,8],length(MLr$userRatingValue))
MLr$R3_the <- rep(w2w[3,8],length(MLr$userRatingValue))
MLr$R4_the <- rep(w2w[4,8],length(MLr$userRatingValue))
MLr$R5_the <- rep(w2w[5,8],length(MLr$userRatingValue))

MLr$R1_they <- rep(w2w[1,9],length(MLr$userRatingValue))
MLr$R2_they <- rep(w2w[2,9],length(MLr$userRatingValue))
MLr$R3_they <- rep(w2w[3,9],length(MLr$userRatingValue))
MLr$R4_they <- rep(w2w[4,9],length(MLr$userRatingValue))
MLr$R5_they <- rep(w2w[5,9],length(MLr$userRatingValue))

MLr$R1_this <- rep(w2w[1,10],length(MLr$userRatingValue))
MLr$R2_this <- rep(w2w[2,10],length(MLr$userRatingValue))
MLr$R3_this <- rep(w2w[3,10],length(MLr$userRatingValue))
MLr$R4_this <- rep(w2w[4,10],length(MLr$userRatingValue))
MLr$R5_this <- rep(w2w[5,10],length(MLr$userRatingValue))

MLr$R1_with <- rep(w2w[1,11],length(MLr$userRatingValue))
MLr$R2_with <- rep(w2w[2,11],length(MLr$userRatingValue))
MLr$R3_with <- rep(w2w[3,11],length(MLr$userRatingValue))
MLr$R4_with <- rep(w2w[4,11],length(MLr$userRatingValue))
MLr$R5_with <- rep(w2w[5,11],length(MLr$userRatingValue))

MLr$R1_you <- rep(w2w[1,12],length(MLr$userRatingValue))
MLr$R2_you <- rep(w2w[2,12],length(MLr$userRatingValue))
MLr$R3_you <- rep(w2w[3,12],length(MLr$userRatingValue))
MLr$R4_you <- rep(w2w[4,12],length(MLr$userRatingValue))
MLr$R5_you <- rep(w2w[5,12],length(MLr$userRatingValue))

MLr$and_diff1 <- MLr$R1_and-MLr$and_ratios
MLr$and_diff2 <- MLr$R2_and-MLr$and_ratios
MLr$and_diff3 <- MLr$R3_and-MLr$and_ratios
MLr$and_diff4 <- MLr$R4_and-MLr$and_ratios
MLr$and_diff5 <- MLr$R5_and-MLr$and_ratios

MLr$but_diff1 <- MLr$R1_but-MLr$but_ratios
MLr$but_diff2 <- MLr$R2_but-MLr$but_ratios
MLr$but_diff3 <- MLr$R3_but-MLr$but_ratios
MLr$but_diff4 <- MLr$R4_but-MLr$but_ratios
MLr$but_diff5 <- MLr$R5_but-MLr$but_ratios

MLr$for_diff1 <- MLr$R1_for-MLr$for_ratios 
MLr$for_diff2 <- MLr$R2_for-MLr$for_ratios 
MLr$for_diff3 <- MLr$R3_for-MLr$for_ratios 
MLr$for_diff4 <- MLr$R4_for-MLr$for_ratios 
MLr$for_diff5 <- MLr$R5_for-MLr$for_ratios 

MLr$good_diff1 <- MLr$R1_good-MLr$good_ratios
MLr$good_diff2 <- MLr$R2_good-MLr$good_ratios
MLr$good_diff3 <- MLr$R3_good-MLr$good_ratios
MLr$good_diff4 <- MLr$R4_good-MLr$good_ratios
MLr$good_diff5 <- MLr$R5_good-MLr$good_ratios

MLr$have_diff1 <- MLr$R1_have-MLr$have_ratios
MLr$have_diff2 <- MLr$R2_have-MLr$have_ratios
MLr$have_diff3 <- MLr$R3_have-MLr$have_ratios
MLr$have_diff4 <- MLr$R4_have-MLr$have_ratios
MLr$have_diff5 <- MLr$R5_have-MLr$have_ratios

MLr$not_diff1 <- MLr$R1_not-MLr$not_ratios
MLr$not_diff2 <- MLr$R2_not-MLr$not_ratios
MLr$not_diff3 <- MLr$R3_not-MLr$not_ratios
MLr$not_diff4 <- MLr$R4_not-MLr$not_ratios
MLr$not_diff5 <- MLr$R5_not-MLr$not_ratios

MLr$that_diff1 <- MLr$R1_that-MLr$that_ratios
MLr$that_diff2 <- MLr$R2_that-MLr$that_ratios
MLr$that_diff3 <- MLr$R3_that-MLr$that_ratios
MLr$that_diff4 <- MLr$R4_that-MLr$that_ratios
MLr$that_diff5 <- MLr$R5_that-MLr$that_ratios

MLr$the_diff1 <- MLr$R1_the-MLr$the_ratios
MLr$the_diff2 <- MLr$R2_the-MLr$the_ratios
MLr$the_diff3 <- MLr$R3_the-MLr$the_ratios
MLr$the_diff4 <- MLr$R4_the-MLr$the_ratios
MLr$the_diff5 <- MLr$R5_the-MLr$the_ratios

MLr$they_diff1 <- MLr$R1_they-MLr$they_ratios
MLr$they_diff2 <- MLr$R2_they-MLr$they_ratios
MLr$they_diff3 <- MLr$R3_they-MLr$they_ratios
MLr$they_diff4 <- MLr$R4_they-MLr$they_ratios
MLr$they_diff5 <- MLr$R5_they-MLr$they_ratios

MLr$this_diff1 <- MLr$R1_this-MLr$this_ratios
MLr$this_diff2 <- MLr$R2_this-MLr$this_ratios
MLr$this_diff3 <- MLr$R3_this-MLr$this_ratios
MLr$this_diff4 <- MLr$R4_this-MLr$this_ratios
MLr$this_diff5 <- MLr$R5_this-MLr$this_ratios

MLr$with_diff1 <- MLr$R1_with-MLr$with_ratios
MLr$with_diff2 <- MLr$R2_with-MLr$with_ratios
MLr$with_diff3 <- MLr$R3_with-MLr$with_ratios
MLr$with_diff4 <- MLr$R4_with-MLr$with_ratios
MLr$with_diff5 <- MLr$R5_with-MLr$with_ratios

MLr$you_diff1 <- MLr$R1_you-MLr$you_ratios
MLr$you_diff2 <- MLr$R2_you-MLr$you_ratios
MLr$you_diff3 <- MLr$R3_you-MLr$you_ratios
MLr$you_diff4 <- MLr$R4_you-MLr$you_ratios
MLr$you_diff5 <- MLr$R5_you-MLr$you_ratios



MLr$andMin <- apply(MLr[74:78],1, min)
MLr$andvote <- ifelse(MLr$R1_and==MLr$andMin,
                    1, 
                    ifelse(MLr$R2_and==MLr$andMin,
                           2,
                           ifelse(MLr$R3_and==MLr$andMin,
                                  3,
                                  ifelse(MLr$R4_and==MLr$andMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$butMin <- apply(MLr[79:83],1, min)
MLr$butvote <- ifelse(MLr$R1_but==MLr$butMin,
                    1, 
                    ifelse(MLr$R2_but==MLr$butMin,
                           2,
                           ifelse(MLr$R3_but==MLr$butMin,
                                  3,
                                  ifelse(MLr$R4_but==MLr$butMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$forMin <- apply(MLr[84:88],1, min)
MLr$forvote <- ifelse(MLr$R1_for==MLr$forMin,
                    1, 
                    ifelse(MLr$R2_for==MLr$forMin,
                           2,
                           ifelse(MLr$R3_for==MLr$forMin,
                                  3,
                                  ifelse(MLr$R4_for==MLr$forMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$goodMin <- apply(MLr[89:93],1, min)
MLr$goodvote <- ifelse(MLr$R1_good==MLr$goodMin,
                    1, 
                    ifelse(MLr$R2_good==MLr$goodMin,
                           2,
                           ifelse(MLr$R3_good==MLr$goodMin,
                                  3,
                                  ifelse(MLr$R4_good==MLr$goodMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$haveMin <- apply(MLr[94:98],1, min)
MLr$havevote <- ifelse(MLr$R1_have==MLr$haveMin,
                    1, 
                    ifelse(MLr$R2_have==MLr$haveMin,
                           2,
                           ifelse(MLr$R3_have==MLr$haveMin,
                                  3,
                                  ifelse(MLr$R4_have==MLr$haveMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$notMin <- apply(MLr[99:103],1, min)
MLr$notvote <- ifelse(MLr$R1_not==MLr$notMin,
                    1, 
                    ifelse(MLr$R2_not==MLr$notMin,
                           2,
                           ifelse(MLr$R3_not==MLr$notMin,
                                  3,
                                  ifelse(MLr$R4_not==MLr$notMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$thatMin <- apply(MLr[104:108],1, min)
MLr$thatvote <- ifelse(MLr$R1_that==MLr$thatMin,
                    1, 
                    ifelse(MLr$R2_that==MLr$thatMin,
                           2,
                           ifelse(MLr$R3_that==MLr$thatMin,
                                  3,
                                  ifelse(MLr$R4_that==MLr$thatMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$theMin <- apply(MLr[109:113],1, min)
MLr$thevote <- ifelse(MLr$R1_the==MLr$theMin,
                    1, 
                    ifelse(MLr$R2_the==MLr$theMin,
                           2,
                           ifelse(MLr$R3_the==MLr$theMin,
                                  3,
                                  ifelse(MLr$R4_the==MLr$theMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$theyMin <- apply(MLr[114:118],1, min)
MLr$theyvote <- ifelse(MLr$R1_they==MLr$theyMin,
                    1, 
                    ifelse(MLr$R2_they==MLr$theyMin,
                           2,
                           ifelse(MLr$R3_they==MLr$theyMin,
                                  3,
                                  ifelse(MLr$R4_they==MLr$theyMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$thisMin <- apply(MLr[119:123],1, min)
MLr$thisvote <- ifelse(MLr$R1_this==MLr$thisMin,
                    1, 
                    ifelse(MLr$R2_this==MLr$thisMin,
                           2,
                           ifelse(MLr$R3_this==MLr$thisMin,
                                  3,
                                  ifelse(MLr$R4_this==MLr$thisMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$withMin <- apply(MLr[124:128],1, min)
MLr$withvote <- ifelse(MLr$R1_with==MLr$withMin,
                    1, 
                    ifelse(MLr$R2_with==MLr$withMin,
                           2,
                           ifelse(MLr$R3_with==MLr$withMin,
                                  3,
                                  ifelse(MLr$R4_with==MLr$withMin,
                                         4,
                                         5)
                                  )
                           )
                    )

MLr$youMin <- apply(MLr[129:133],1, min)
MLr$youvote <- ifelse(MLr$R1_you==MLr$youMin,
                    1, 
                    ifelse(MLr$R2_you==MLr$youMin,
                           2,
                           ifelse(MLr$R3_you==MLr$youMin,
                                  3,
                                  ifelse(MLr$R4_you==MLr$youMin,
                                         4,
                                         5)
                                  )
                           )
                    )


```

```{r}
bestVote <- MLr %>% select(andvote, 
      butvote, forvote, 
      goodvote, havevote,
      notvote,  thatvote,  thevote,theyvote,
      thisvote,withvote,
      youvote)
      
bestVote$andvote <- as.factor(paste(bestVote$andvote)) 
bestVote$butvote <- as.factor(paste(bestVote$butvote)) 
bestVote$forvote <- as.factor(paste(bestVote$forvote)) 
bestVote$goodvote <- as.factor(paste(bestVote$goodvote)) 
bestVote$havevote <- as.factor(paste(bestVote$havevote))
bestVote$notvote <- as.factor(paste(bestVote$notvote))  
bestVote$thatvote <- as.factor(paste(bestVote$thatvote))                                  
bestVote$thevote <- as.factor(paste(bestVote$thevote))
bestVote$theyvote <- as.factor(paste(bestVote$theyvote))
bestVote$thisvote <- as.factor(paste(bestVote$thisvote))
bestVote$withvote <- as.factor(paste(bestVote$withvote))
bestVote$youvote <- as.factor(paste(bestVote$youvote))

bestVote$counts1 <- 0
bestVote$counts2 <- 0
bestVote$counts3 <- 0
bestVote$counts4 <- 0
bestVote$counts5 <- 0

a5 <- grep('5',bestVote$andvote)
a4 <- grep('4', bestVote$andvote)
a3 <- grep('3',bestVote$andvote)
a2 <- grep('2',bestVote$andvote)
a1 <- grep('1',bestVote$andvote)

b5 <- grep('5',bestVote$butvote)
b4 <- grep('4', bestVote$butvote)
b3 <- grep('3',bestVote$butvote)
b2 <- grep('2',bestVote$butvote)
b1 <- grep('1',bestVote$butvote)

c5 <- grep('5',bestVote$forvote)
c4 <- grep('4', bestVote$forvote)
c3 <- grep('3',bestVote$forvote)
c2 <- grep('2',bestVote$forvote)
c1 <- grep('1',bestVote$forvote)

d5 <- grep('5',bestVote$goodvote)
d4 <- grep('4', bestVote$goodvote)
d3 <- grep('3',bestVote$goodvote)
d2 <- grep('2',bestVote$goodvote)
d1 <- grep('1',bestVote$goodvote)

e5 <- grep('5',bestVote$havevote)
e4 <- grep('4', bestVote$havevote)
e3 <- grep('3',bestVote$havevote)
e2 <- grep('2',bestVote$havevote)
e1 <- grep('1',bestVote$havevote)

f5 <- grep('5',bestVote$notvote)
f4 <- grep('4', bestVote$notvote)
f3 <- grep('3',bestVote$notvote)
f2 <- grep('2',bestVote$notvote)
f1 <- grep('1',bestVote$notvote)

g5 <- grep('5',bestVote$thatvote)
g4 <- grep('4', bestVote$thatvote)
g3 <- grep('3',bestVote$thatvote)
g2 <- grep('2',bestVote$thatvote)
g1 <- grep('1',bestVote$thatvote)

h5 <- grep('5',bestVote$thevote)
h4 <- grep('4', bestVote$thevote)
h3 <- grep('3',bestVote$thevote)
h2 <- grep('2',bestVote$thevote)
h1 <- grep('1',bestVote$thevote)

i5 <- grep('5',bestVote$theyvote)
i4 <- grep('4', bestVote$theyvote)
i3 <- grep('3',bestVote$theyvote)
i2 <- grep('2',bestVote$theyvote)
i1 <- grep('1',bestVote$theyvote)

j5 <- grep('5',bestVote$thisvote)
j4 <- grep('4', bestVote$thisvote)
j3 <- grep('3',bestVote$thisvote)
j2 <- grep('2',bestVote$thisvote)
j1 <- grep('1',bestVote$thisvote)

k5 <- grep('5',bestVote$withvote)
k4 <- grep('4', bestVote$withvote)
k3 <- grep('3',bestVote$withvote)
k2 <- grep('2',bestVote$withvote)
k1 <- grep('1',bestVote$withvote)

l5 <- grep('5',bestVote$youvote)
l4 <- grep('4', bestVote$youvote)
l3 <- grep('3',bestVote$youvote)
l2 <- grep('2',bestVote$youvote)
l1 <- grep('1',bestVote$youvote)

bestVote$counts1[l1] <- bestVote$counts1[l1]+ 1
bestVote$counts1[k1] <- bestVote$counts1[k1]+ 1
bestVote$counts1[j1] <- bestVote$counts1[j1]+ 1
bestVote$counts1[i1] <- bestVote$counts1[i1]+ 1
bestVote$counts1[h1] <- bestVote$counts1[h1]+ 1
bestVote$counts1[g1] <- bestVote$counts1[g1]+ 1
bestVote$counts1[f1] <- bestVote$counts1[f1]+ 1
bestVote$counts1[e1] <- bestVote$counts1[e1]+ 1
bestVote$counts1[d1] <- bestVote$counts1[d1]+ 1
bestVote$counts1[c1] <- bestVote$counts1[c1]+ 1
bestVote$counts1[b1] <- bestVote$counts1[b1]+ 1
bestVote$counts1[a1] <- bestVote$counts1[a1]+ 1

bestVote$counts2[l2]  <- bestVote$counts2[l2] + 1
bestVote$counts2[k2]  <- bestVote$counts2[k2] + 1
bestVote$counts2[j2]  <- bestVote$counts2[j2] + 1
bestVote$counts2[i2]  <- bestVote$counts2[i2] + 1
bestVote$counts2[h2]  <- bestVote$counts2[h2] + 1
bestVote$counts2[g2]  <- bestVote$counts2[g2] + 1
bestVote$counts2[f2]  <- bestVote$counts2[f2] + 1
bestVote$counts2[e2]  <- bestVote$counts2[e2] + 1
bestVote$counts2[d2]  <- bestVote$counts2[d2] + 1
bestVote$counts2[c2]  <- bestVote$counts2[c2] + 1
bestVote$counts2[b2]  <- bestVote$counts2[b2] + 1
bestVote$counts2[a2]  <- bestVote$counts2[a2] + 1

bestVote$counts3[l3]  <- bestVote$counts3[l3] + 1
bestVote$counts3[k3]  <- bestVote$counts3[k3] + 1
bestVote$counts3[j3]  <- bestVote$counts3[j3] + 1
bestVote$counts3[i3]  <- bestVote$counts3[i3] + 1
bestVote$counts3[h3]  <- bestVote$counts3[h3] + 1
bestVote$counts3[g3]  <- bestVote$counts3[g3] + 1
bestVote$counts3[f3]  <- bestVote$counts3[f3] + 1
bestVote$counts3[e3]  <- bestVote$counts3[e3] + 1
bestVote$counts3[d3]  <- bestVote$counts3[d3] + 1
bestVote$counts3[c3]  <- bestVote$counts3[c3] + 1
bestVote$counts3[b3]  <- bestVote$counts3[b3] + 1
bestVote$counts3[a3]  <- bestVote$counts3[a3] + 1

bestVote$counts4[l4]  <- bestVote$counts4[l4] + 1
bestVote$counts4[k4]  <- bestVote$counts4[k4] + 1
bestVote$counts4[j4]  <- bestVote$counts4[j4] + 1
bestVote$counts4[i4]  <- bestVote$counts4[i4] + 1
bestVote$counts4[h4]  <- bestVote$counts4[h4] + 1
bestVote$counts4[g4]  <- bestVote$counts4[g4] + 1
bestVote$counts4[f4]  <- bestVote$counts4[f4] + 1
bestVote$counts4[e4]  <- bestVote$counts4[e4] + 1
bestVote$counts4[d4]  <- bestVote$counts4[d4] + 1
bestVote$counts4[c4]  <- bestVote$counts4[c4] + 1
bestVote$counts4[b4]  <- bestVote$counts4[b4] + 1
bestVote$counts4[a4]  <- bestVote$counts4[a4] + 1

bestVote$counts5[l5]  <- bestVote$counts5[l5] + 1
bestVote$counts5[k5]  <- bestVote$counts5[k5] + 1
bestVote$counts5[j5]  <- bestVote$counts5[j5] + 1
bestVote$counts5[i5]  <- bestVote$counts5[i5] + 1
bestVote$counts5[h5]  <- bestVote$counts5[h5] + 1
bestVote$counts5[g5]  <- bestVote$counts5[g5] + 1
bestVote$counts5[f5]  <- bestVote$counts5[f5] + 1
bestVote$counts5[e5]  <- bestVote$counts5[e5] + 1
bestVote$counts5[d5]  <- bestVote$counts5[d5] + 1
bestVote$counts5[c5]  <- bestVote$counts5[c5] + 1
bestVote$counts5[b5]  <- bestVote$counts5[b5] + 1
bestVote$counts5[a5]  <- bestVote$counts5[a5] + 1


bestVote$maxVote <- apply(bestVote[13:17],1,max)

mv <- bestVote$maxVote
ct1 <- bestVote$counts1
ct2 <- bestVote$counts2
ct3 <- bestVote$counts3
ct4 <- bestVote$counts4
ct5 <- bestVote$counts5


bestVote$votedRating <- ifelse(mv==ct1, 1, 
                        ifelse(mv==ct2, 2,
                        ifelse(mv==ct3, 3,
                        ifelse(mv==ct4, 4, 5))))

bestVote$Rating <- ifelse(mv==ct1 & (mv==ct2|mv==ct3|mv==ct4|mv==ct5),'tie',
                     ifelse(mv==ct1,1, 
                           ifelse(mv==ct2 & (mv==ct3|mv==ct4|mv==ct5),'tie',
                     ifelse(mv==ct2, 2,
                           ifelse(mv==ct3 &(mv==ct4|mv==ct5),'tie', 
                           ifelse(mv==ct3, 3,
                           ifelse(mv==ct4 & mv==ct5, 'tie',
                           ifelse(mv==ct4, 4,5
                           )))))))
                          )
bestVote$finalPrediction <- ifelse(bestVote$Rating=='tie', 
                     ifelse(ceiling((c(ct1,ct2,ct3,ct4,ct5)*c(1,2,3,4,5))/5) > 5,
                     5, ceiling((c(ct1,ct2,ct3,ct4,ct5)*c(1,2,3,4,5))/5)), 
                     bestVote$votedRating )

```


Now that we have our final prediction with this algorithm. Lets attach these two tables together and rearrange the columns.
```{r}
MLr2 <- cbind(MLr, bestVote)
MLr3 <- MLr2[,c(2:178,1)]
MLr3$CorrectPrediction <- ifelse(MLr3$finalPrediction==MLr3$userRatingValue,
                                 1,0)
MLr3$finalPrediction <- as.factor(paste(MLr3$finalPrediction))
MLr3$userRatingValue <- paste('rating ', MLr3$userRatingValue,sep='')
Accuracy <- sum(MLr3$CorrectPrediction)/length(MLr3$CorrectPrediction)
Accuracy
```

The first bunch of reviews seem to be very accurate, but then more reviews get less accurate. 
```{r}
MLr3[1:20,175:179]
```

This was a good approach to building a model from the ground up and developing algorithms that seem like they could be good models to make predictions with. This was a straight algorithm model as in a function you put something in and get an answer out. The caret package has other more heavily used and industry related models tht are used frequently and can be tune. We will look at those later.

Lets write this table out to csv.
```{r}
write.csv(MLr3, 'ML_Reviews614_resultsTable.csv', row.names=FALSE)
```


Lets look at those values that the correct prediction was false.
```{r}
FalsePredictions <- subset(MLr3, MLr3$CorrectPrediction==0)
head(FalsePredictions[,175:179],30)
```
Many times the voted rating was not the rating even if it wasn't a tie. The 2-4 ratings were most of the incorrect predictions and the rating as being either a 5 or a 1 were more 50/50 probably because their ratios of term to total terms were very close. Some tuning that could be done would be to select different keywords, see if there is a difference between type of business or cost of service or goods. We left in these words of which most are included in stopwords like by,my,has,have,etc.that are personal pronouns or  of the text mining tm package. 
```{r}
#library(tm)
stop <- stopwords()
stop
```
The above are the stopwords that are eliminated within text cleaning and preprocessing before running or retrieving a document term matrix for an observation such as a review.
Now lets look at are keywords.
```{r}
keywordsUsed <- colnames(wordToAllWords)
keywordsUsed
```
The above shows our keywords, and note that for is for_ because it errors in R as it is a programming keyword so it was altered in the column name with an appended underscore character, but the search for it as a character or factor is 'for' not 'for_'. We actually see that all 12 of our stopwords are keywords. We briefly explained that this is because in grammar and composition to write a persuasive stories many connections have to be made with the use of pronouns and actions and states of being and descriptors. These were used as features by count to see if they added any value to predicting a review. We do see that these stop words are used a lot for extreme ends of the rating scale as a 1 for the lowest and 5 for the highest review rating.

Lets get a count of each rating by incorrect classification.
```{r}
userRatings <- FalsePredictions %>% group_by(userRatingValue) %>% count(finalPrediction)
userRatings
```
We can see from the userRatingValue and the final prediction, that there were 85 instances
where this model couldn't distinguish between a 1 or 5 rating based on the keywords (mostly stopwords). But also those ratings that were 4s were actually classified the most incorrectly as a 5 because some people don't readily give 5s as others. Also the actual 2s that were classified incorrectly were scaled up to 5s in the error, the same with the 3s. The gray areas were in user ratings of 2-4, where few were classified as a value in a 2-4 that was incorrect. If we instead said to classify any 2-4 as in the range of 2-4 without penalizing the exact value, then the prediction accuracy would be better. But people have their own reasons for giving 2-4s in ratings. Like they had better or there was an absolute worst experience the company still doesn't meet because he or she knows it could be worse and it could be better. Companies are supposed to use this to improve or adjust needs of consumers. But at the same time some users are just upset with the cost or time wasted or spent when other options they know of were or are better.

Lets look at those predictions with a tie.
```{r}
ties <- subset(MLr3, MLr3$Rating=='tie')
ties[,175:179]
```
There weren't a lot of ties on votes for the minimum difference of review to all reviews by ratios of term to total terms.
```{r}
tieGroups <- ties %>% group_by(userRatingValue) %>% count(finalPrediction)
tieGroups
```

Note that these tie ratings are both correct and incorrect. From the above grouped information of counts of user ratings by final predictions all the 5s were misclassified or predicted to be in the gray area of 2-4 and more of the 1s were classified correctly except for one review in the middle gray area. None of the 2s are in our list of ties but many of the gray area 2-4s were classified into the gray area of incorrect 2-4s such as the 4s and 3s.

We could use a link analysis to see how these results compared with the final predicted value and actual rating value as groups. With the nodes as the user rating, and edges as the final prediction. We should also add in one of either the business type or the cost.
First lets combine the Reviews15 table with the MLr3 table of only the results.
```{r}
MLr4 <- MLr3 %>% select(maxVote:CorrectPrediction)
MLr4$actualRatingValue <- MLr4$userRatingValue
MLr5 <- MLr4 %>% select(-userRatingValue)
Reviews15_results <- cbind(Reviews15, MLr5)
colnames(Reviews15_results)
```
Lets keep the ratios because this table minus results (and the review content columns) just added will be useful when running the caret algorithms. For now, lets write this out to csv to easily read in later instead of running whatever chunks preceded this table to get it.
```{r}
write.csv(Reviews15_results, 'Reviews15_results.csv', row.names=FALSE)
```

Now lets use visNetwork to group by CorrectPrediction and map the actualRatingValue to the predicted values as well as add a hovering feature as the title column in our nodes table of the businessType. We have to pick a width for the weighted arrows, or else there won't be any edges. Lets pick the not_ratios. 
```{r}
network <- Reviews15_results %>% select(businessType,
                                        finalPrediction:actualRatingValue, not,
                                        not_ratios)

network$finalPrediction <- paste('predicted',network$finalPrediction,sep=' ')
network$CorrectPrediction <- gsub(1,'TRUE', network$CorrectPrediction)
network$CorrectPrediction <- gsub(0,'FALSE', network$CorrectPrediction)
head(network,20)
```

The nodes table will include all of the above except the not_ratios.
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$title <- nodes$businessType
nodes$label <- nodes$actualRatingValue
nodes$group <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label, title,group,finalPrediction)
head(nodes1,10)
```

The edges will have the finalPrediction and actualRatingValue columns.
```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(finalPrediction,actualRatingValue,not,not_ratios)
edges$label <- edges$finalPrediction
edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$actualRatingValue,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$finalPrediction,
                                                   from = nodes$finalPrediction,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width,weight)
head(edges3,10)
```

Lets see the link analysis visualization.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions of True or False Ratings') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```

The above shows a visual network that looks mostly half and half by true versus false predicted rating values.None of the edges show except for that tiny cluster above, that shows the predicted ratings.


Lets group by business type now and map the actual to correctly predicted
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$group <- nodes$businessType
nodes$label <- nodes$actualRatingValue
nodes$title <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label, title,group,finalPrediction)
head(nodes1,10)
```

The edges will have the finalPrediction and actualRatingValue columns.
```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(finalPrediction,actualRatingValue,not,not_ratios)
edges$label <- edges$finalPrediction
edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$actualRatingValue,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$finalPrediction,
                                                   from = nodes$finalPrediction,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width, weight)
head(edges3,10)
```

Lets see the link analysis visualization.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions of Business Type Ratings with True or False and Actual Rating') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout() %>%
  visLegend
```


Lets group by business type now and map the business type to the prediction.
```{r,error=FALSE,message=FALSE, warning=FALSE}
nodes <- network
nodes$id <- row.names(nodes)
nodes$group <- nodes$businessType
nodes$label <- nodes$businessType
nodes$title <- nodes$CorrectPrediction
nodes1 <- nodes %>% select(id, label,group,finalPrediction, title)
head(nodes1,10)
```

```{r,error=FALSE,message=FALSE, warning=FALSE}
edges <- network %>% select(CorrectPrediction,actualRatingValue,not,businessType,
                            finalPrediction)
edges$label <- edges$finalPrediction
#edges$weight <- edges$not_ratios
edges$width <- edges$not
edges1 <- edges %>% mutate(from = plyr::mapvalues(edges$businessType,
                                                  from = nodes$label, 
                                                  to = nodes$id)
                           )
edges2 <- edges1 %>% mutate(to = plyr::mapvalues(edges$CorrectPrediction,
                                                   from = nodes$title,
                                                   to = nodes$id)
                            )
edges3 <- edges2 %>% select(from,to,label,width)
head(edges3,10)
```

Lets see the link analysis visualization with the star layout.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout.star') %>%
  visLegend
```
The above is a hula hoop because there are not any links for the most part, and only a handfule of the true or false predictions are mapping from business type to other business types by other business types predicted true or false. you can click on a node drag it off the disk and see no links, but click on the background to stop dragging. Then do the same with the center nodes that do have links to the disk and see the edges stay attached with the predicted value. Not many business types had the same predicted value as a true or false prediction of the same actual to predicted by business type.

Lets see this design in a grid layout. The sphere is the default and you can see it doesn't really describe much visually without grouping when there are no links between the nodes.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout_on_grid') %>%
  visLegend
```

The above is a grid layout that looks like tetris or pac man ping pong machine type layout with the groups.

Lets look out the layout.graphopt layout next.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout.graphopt') %>%
  visLegend
```

Looks a lot like the default layout for this data of features. This is the spherical layout that follows.
```{r}
visNetwork(nodes=nodes1, edges=edges3, main='Grouped Predictions by Business Type') %>% visEdges(arrows=c('from','middle')) %>%
  visInteraction(navigationButtons=TRUE, dragNodes=TRUE,
                 dragView=TRUE, zoomView = TRUE) %>%
  visOptions(nodesIdSelection = TRUE, manipulation=FALSE) %>%
  visIgraphLayout(layout='layout_on_sphere') %>%
  visLegend
```

It looks similar to the hula hoop star layout for this data with minimal links between nodes.

***
***
***
 That was interesting to look at the different layouts in igraph with visNetwork, and to also see ways to improve our model design just as the businesses use reviews to improve or make changes as needed by analyzing the correctly and falsely predicted ratings. The next section will use machine learning algorithms in the 
 [caret package](https://github.com/rstudio/cheatsheets/raw/master/caret.pdf)to test out results of various models and defaults as well as test out the attributes within each model for tuning and validating for better generalization. 