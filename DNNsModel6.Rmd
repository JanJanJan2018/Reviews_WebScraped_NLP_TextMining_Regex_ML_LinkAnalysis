---
title: "v3 DNNs soc.med. reviews"
author: "Janis Corona"
date: "5/4/2020"
output: html_document
---

Lets look at the table of Reviews from four businesses made up of a high end spa, a low cost grocery store, and two different chiropractors' offices in or around Corona, CA.  This is based on the cleaned up reviews of these businesses from a social media site where cleaned up just means filtering out business ads, the date, the user photos, user rating, etc. from the header of each review. Only the columns that were needed were used and saved as 'userReviewBusinessTypeRatig.csv'. 
Tensorflow's Keras is used to build model6 DNN with hundreds of units in approximately 10 hidden layers with mostly Relu activation, a learning rate of 0.01 and increased momentum of 1.5 with decay decreased.
This script uses Model 6 for 5 epochs and with many hidden layers with the first hidden layers having 10,000+ units, but not more than the number of features of 52,000+ features.And takes a long time to run even on a HP PC using Windows with 16 GBs memory and an AMD 5 compute core 2C+3G 2.6 GHz processor. Tensorflow is used but for the keras module for running this deep neural network. 

This will be set aside for another day, when I start using GPUs because it is very slow. It could be the ReLU activations that get rid of vanishing gradients the tanh and sigmoid fall into as traps that slow down the loss function or optimization., because they [ReLUs] aren't continuous, and there are thousands of units and 10 or more layers. If I do get a laptop with a GPU interface, this will be nice to see how fast the GPUs can process this, because this computer with the above details is not able to process it in under a couple hours, too long for me to wait. It does one epoch in Jupyter notebook and freezes up, doesn't display results.
```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 

```


```{r}
use_condaenv(condaenv = "python36")

```


# Deep Neural Net Models


```{python}
import numpy as np
import pandas as pd

```


# Data Normalization and Splits
```{python}
data = pd.read_csv('lemm123gramsHealthWellness.csv', encoding='unicode_escape')

```


```{python}
data.shape

```


```{python}
data.head()

```


```{python}
np.random.seed(123)
data0 = data.reindex(np.random.permutation(data.index))

```


```{python}
data1 = data0.iloc[:,3:]

```


```{python}
data1.shape

```


```{python}
target=data0.iloc[:,0:1]

```


```{python}
target.shape

```


```{python}
print(target['Rating'].unique())
print(len(target['Rating'].unique()))

```


```{python}
mean_vals = np.mean(data1, axis=0)
std_val = np.std(data1)

data1_centered = (data1 - mean_vals)/std_val

print(data1_centered.shape, target.shape)

```


```{python}
print(data1.head())

```


```{python}
print(target.head())

```

These values for rating are already integer, so this next step and the one that follows isn't needed, but we could just run it anyways.
```{python}
#numpy function

class_mapping = {label: idx for idx, label in enumerate(np.unique(target['Rating']))}
class_mapping

```

The mapping mapped ratings 1-5 into 0:4 values.
```{python}
target['OH_rating']=target['Rating']
target['OH_rating'] = target['Rating'].map(class_mapping)
target.head()

```


```{python}
target1 = target['OH_rating']
target1.head()

```

Testing split on data with permutated indices. There are 614 instances in this data, 20% is about 123, and 80% is about 491 instances.
```{python}
X_train = data1[:491]
X_test = data1[491:]
y_train = target1[:491]
y_test = target1[491:]

################################
# for adding the names of the classes after prediction from earlier in script
y_trainNames = target['Rating']
y_trainNames = y_trainNames[:491]
y_trainNames.columns=['Rating']
y_trainNames1=pd.DataFrame(y_trainNames)

y_testNames = target['Rating']
y_testNames = y_testNames[491:]
y_testNames.columns=['Rating']
y_testNames1=pd.DataFrame(y_testNames)
################################

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
```


```{python}
y_train

```


```{python}
import tensorflow as tf
import tensorflow.contrib.keras as keras
#optionally use import tensorflow.keras as keras when no longer experimental contributor package development

np.random.seed(123)
tf.set_random_seed(123)

```


# DNN Model6

Lets try increasing the momentum and decreasing the learning rate and increasing the decay parameters in the optimization part of the model and see what happens to model5.
```{python}
model6 = keras.models.Sequential()

model6.add(
    keras.layers.Dense(
        units=20000,   #output units need to match next layer inputs 
        input_dim=52611, #number of features for input above says 52611
        kernel_initializer='glorot_uniform',# name of the guy behind Xavier Initialization; the biases to zero
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=10000,   #output matches next layer input 
        input_dim=20000, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))

model6.add(
    keras.layers.Dense(
        units=500,   #output matches next layer input 
        input_dim=10000, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='tanh'))


model6.add(
    keras.layers.Dense(
        units=400,   #output matches next layer input 
        input_dim=500, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))


model6.add(
    keras.layers.Dense(
        units=200,   #output matches next layer input 
        input_dim=400, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))


model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=200, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))



model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=100, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))

model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=100, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))


model6.add(
    keras.layers.Dense(
        units=100,   #output matches next layer input 
        input_dim=100, #input matches last layer's output
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='relu'))


model6.add(
    keras.layers.Dense(
        units=19,  #these are the number of class categories in our target  
        input_dim=100,
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='softmax'))#will return the class membership probs summing to 1 of all class probs

# these are hyperparameters that can be tuned if overfitting during training, or to get better accuracy
sgd_optimizer = keras.optimizers.SGD( 
        lr=0.01, decay=1e-5, momentum=1.5)

# categorical_crossentropy is used in multiclass classification instead of binary_crossentropy
# to match the softmax function
model6.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy')
# it was 'categorical_crossentropy', but that expects binary matrices of 1s and 0s
# it said to use sparse_categorical_crossentropy

```
The above has 10 layers, with an added sigmoid layer, and one tanh as the early hidden layers, but the rest are the relu layers.


```{python}
history6 = model6.fit(X_train, y_train,
                    batch_size=50, epochs=5,
                    verbose=1, 
                    validation_split=0.1) 

```

```{python}
y_train_pred6 = model6.predict_classes(X_train, verbose=0)
print('First 3 predictions: ', y_train_pred6[:3])

```

```{python}
y_train_pred6 = model6.predict_classes(X_train, 
                                     verbose=0)

```

```{python}
y_train_pred6 = pd.DataFrame(y_train_pred6)
y_train_pred6.columns=['predicted']

y_train6 = y_train
y_train6 = pd.DataFrame(y_train6)
y_train6.columns=['OH_Rating']

y_train_pred6.index=y_train6.index

Train6=pd.concat([y_train6['OH_Rating'],y_trainNames1['Rating'],y_train_pred6['predicted']],axis=1)

print(Train6)

```
The true rating is the ratings on a different scale, because we chose to use the mapping that was designed for categorical values having string factor names instead of integer digits. 

```{python}
y_test_pred6 = model6.predict_classes(X_test, 
                                    verbose=0)

```

```{python}
y_test_pred6 = pd.DataFrame(y_test_pred6)
y_test_pred6.columns=['predicted']

y_test6 = y_test
y_test6 = pd.DataFrame(y_test6)
y_test6.columns=['OH_Rating']

y_test_pred6.index=y_test6.index

Test6=pd.concat([y_test6['OH_Rating'],y_testNames1['Rating'],y_test_pred6['predicted']],axis=1)

print(Test6)

```

# DNN Model6 Results 
```{python}
s = sum(Train6['OH_Rating']==Train6['predicted'])
l = len(Train6['Rating'])
accTrain6 = s/l
print('Training Correctly Predicted:',s,'Training Accuracy:',accTrain6,'\n')

```
Thats pretty good accuracy on these very mixed ratings from various business models.

```{python}
s = sum(Test6['OH_Rating']==Test6['predicted'])
l = len(Test6['OH_Rating'])
accTest6 = s/l
print('Testing Correctly Predicted:',s,'Testing Accuracy:',accTest6)

```


